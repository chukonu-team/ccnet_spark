{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86aaf72e-5fba-43eb-b10c-31aa56d42206",
   "metadata": {},
   "source": [
    "# ccnet spark pipeline 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e7aa31-b893-45a3-aa85-450f056caf1f",
   "metadata": {},
   "source": [
    "## 1. 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad5b292-4eb3-4cc0-8837-2a8cf03fd7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 17:40:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from ccnet_spark import open_read, parse_warc_file,compute_hashes,NaiveHashSet, text_normalizer\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StringType,IntegerType,StructType, StructField\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "from cachetools import cached ### model 缓存\n",
    "\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"CCNETSpark\")  \\\n",
    "                    .config(\"spark.executor.memory\", \"100g\") \\\n",
    "                    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "                    .config(\"spark.driver.maxResultSize\", \"32g\") \\\n",
    "                    .config('spark.sql.execution.arrow.enabled', 'true') \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99a057-a8db-4d94-b772-988434ac58cc",
   "metadata": {},
   "source": [
    "## 2. 读取文件数据，处理成pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493e968-ee52-488d-bab9-5c54cff63dd6",
   "metadata": {},
   "source": [
    "### 2.1 获取cache文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb17ca6-b75b-4307-8083-d5e48e5cd768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../cache_data/2019-09/CC-MAIN-20190215183319-20190215205319-00003.warc.wet.gz\n"
     ]
    }
   ],
   "source": [
    "cache_data=\"../cache_data/2019-09/\"\n",
    "def getWETURL(segment: int):\n",
    "    cache_file_prefix = \"CC-MAIN-20190215183319-20190215205319-\"\n",
    "    cache_file_sufix = \".warc.wet.gz\"\n",
    "    segment_str = str(segment).zfill(5)  # Pad with leading zeros\n",
    "    return cache_data+cache_file_prefix + segment_str + cache_file_sufix\n",
    "url = getWETURL(3)\n",
    "print(url)  # Output: CC-MAIN-20190215183319-20190215205319-00003.warc.wet.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0530fda-e712-49b4-bab1-d9c76972b465",
   "metadata": {},
   "source": [
    "### 2.2 处理文件，存入pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed3705e-128e-4f62-b4b8-f09dce29e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getpdf(segment,isPart:bool):\n",
    "    file_path=Path(getWETURL(segment))\n",
    "    file=open_read(file_path)\n",
    "    s=time.time()\n",
    "    pandas_df = parse_warc_file(file, 30)\n",
    "    if(isPart):\n",
    "        random_save_n=100\n",
    "        pandas_df = pandas_df.sample(n=random_save_n, random_state=1)\n",
    "    e=time.time()\n",
    "    print(f\"====== parse segment:{segment} to pd_df consume:{e-s} s\")\n",
    "    return pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e0145-8660-4e4d-ac8a-aab2fc41a16c",
   "metadata": {},
   "source": [
    "## 3. 读取 spark dataframe 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cefa63a6-47df-447c-8c36-f780233a16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsdf(segment,isPart:bool):\n",
    "    inner_path = \"_part\" if isPart else \"_all\"\n",
    "    output_path = cache_data+\"cache_parquet/\"+str(segment)+  inner_path +\".parquet\"  # 设置输出路径\n",
    "    # 检查本地文件是否存在\n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"======process to parquet of segment {segment}{inner_path}\")\n",
    "        # 处理文件并生成 Spark DataFrame\n",
    "        pdf = getpdf(segment,isPart=isPart)\n",
    "        pdf.to_parquet(output_path)  # 保存为 parquet 文件\n",
    "        spark_df = spark.createDataFrame(pdf)\n",
    "    else:\n",
    "        print(f\"======read parquet of segment {segment}{inner_path} from cache\")\n",
    "        pdf = pd.read_parquet(output_path)\n",
    "        spark_df = spark.createDataFrame(pdf)\n",
    "    return spark_df\n",
    "def getsdfs(segments,isPart:bool = False):\n",
    "    merged_sdf=None\n",
    "    for seg in segments:\n",
    "        if(merged_sdf):\n",
    "            merged_sdf = merged_sdf.unionAll(getsdf(seg,isPart)) # Merge DataFrames\n",
    "        else:\n",
    "            merged_sdf = getsdf(seg,isPart)\n",
    "    return merged_sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef42772d-a0cd-44e7-b197-312deb3ef263",
   "metadata": {},
   "source": [
    "### 3.1 load spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "baf88a49-9535-4646-ac40-b2d23d16c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModePara(mode):\n",
    "    if(mode==\"test\"):\n",
    "        para={\n",
    "            \"isTest\":True,\n",
    "            \"isPart\":True,\n",
    "            \"segments\":5,\n",
    "        }\n",
    "        return para\n",
    "    else:\n",
    "        para={\n",
    "            \"isTest\":False,\n",
    "            \"isPart\":False,\n",
    "            \"segments\":40,\n",
    "        }\n",
    "        return para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac2930d2-fa9b-420f-bc2d-5fc18322fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode=\"test\"\n",
    "mode_para=getModePara(mode)\n",
    "segments=[i for i in range(mode_para[\"segments\"])]\n",
    "isPart=mode_para[\"isPart\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12719579-006a-4e30-bdad-a11cd62340b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======read parquet of segment 0_part from cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 17:40:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/04/01 17:40:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/04/01 17:40:30 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======read parquet of segment 1_part from cache\n",
      "======read parquet of segment 2_part from cache\n",
      "======read parquet of segment 3_part from cache\n",
      "======read parquet of segment 4_part from cache\n",
      "load 5 segments,with 5000 docs,comsume:3.980591297149658s\n"
     ]
    }
   ],
   "source": [
    "s=time.time()\n",
    "spark_df = getsdfs(segments,isPart=isPart)\n",
    "num_docs=spark_df.count()\n",
    "e=time.time()\n",
    "print(f\"load {len(segments)} segments,with {num_docs} docs,comsume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e1f0c-f711-4b54-a2f4-7317326414ed",
   "metadata": {},
   "source": [
    "### 3.2 字段分析\n",
    "1. wet 文件本身带有长度：\"length\": length,这个是从wet的\"Content-Length:\"读出来的，和我计算len(raw_content）有出入。考虑原因是原先的length不只是说raw_content，还包括title等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c10dbbad-2058-4308-91c4-74edfee456e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "DataFrame[summary: string, url: string, date_download: string, digest: string, length: string, nlines: string, source_domain: string, title: string, raw_content: string]\n",
      "+--------------------+------+------+--------------+\n",
      "|                 url|length|nlines|compute_length|\n",
      "+--------------------+------+------+--------------+\n",
      "|https://www.telel...|  4758|   111|          4669|\n",
      "|http://www.ma.hu/...|  4180|    70|          3716|\n",
      "|http://angagement...|  1326|    65|          1231|\n",
      "|http://resistther...|   912|    23|           868|\n",
      "|http://klimadiagr...|  1918|    51|          1851|\n",
      "+--------------------+------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:0.6367359161376953s\n"
     ]
    }
   ],
   "source": [
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(spark_df.summary())\n",
    "    tmp_df = spark_df.withColumn(\"compute_length\", F.length(spark_df[\"raw_content\"]))\n",
    "    tmp_df.select(\"url\",\"length\",\"nlines\",\"compute_length\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d8ff6e-69db-4412-92c5-a4783623c124",
   "metadata": {},
   "source": [
    "### 3.3 修改length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b05383d-fc9d-4af6-b8be-7b74ce80ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df=spark_df.withColumn(\"length\", F.length(spark_df[\"raw_content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9793de8-008f-4aec-9dc7-0bbad27feff3",
   "metadata": {},
   "source": [
    "## 4. hash计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dea9e5-b3a8-4b50-b40c-147fd7bbecbd",
   "metadata": {},
   "source": [
    "### 4.1 定义UDF,将doc 分割成paragraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44be1b10-248f-470d-874b-3b3f4962a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，用于分割文本\n",
    "def split_raw_content(content):\n",
    "    lines = content.split('\\n')\n",
    "    line_ids = range(0, len(lines))  # 生成行号\n",
    "    return list(zip(line_ids, lines))\n",
    "\n",
    "# 注册为UDF\n",
    "split_udf = udf(split_raw_content, ArrayType(StructType([\n",
    "    StructField(\"raw_line_id\", IntegerType(), False),\n",
    "    StructField(\"raw_line\", StringType(), False)\n",
    "])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac16adaa-b1c1-4e99-8bdf-93312621e5f4",
   "metadata": {},
   "source": [
    "### 4.2 udf 处理添加新字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c505bf4-b9f4-4e1f-ad51-ad56939b28a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "DataFrame[summary: string, url: string, date_download: string, digest: string, length: string, nlines: string, source_domain: string, title: string, raw_content: string]\n",
      "+--------------------+------+------+--------------------+--------------------+\n",
      "|                 url|length|nlines|         raw_content|       split_content|\n",
      "+--------------------+------+------+--------------------+--------------------+\n",
      "|https://www.telel...|  4669|   111|English\\tEnglish\\...|[{0, English\\tEng...|\n",
      "|http://www.ma.hu/...|  3716|    70|hirdetés\\nma.hu n...|[{0, hirdetés}, {...|\n",
      "|http://angagement...|  1231|    65|→ по-русски\\nCost...|[{0, → по-русски}...|\n",
      "|http://resistther...|   868|    23|Unwanted Resistan...|[{0, Unwanted Res...|\n",
      "|http://klimadiagr...|  1851|    51|Das Klima in Karl...|[{0, Das Klima in...|\n",
      "+--------------------+------+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:1.0078210830688477s\n"
     ]
    }
   ],
   "source": [
    "# 假设spark_df是您的DataFrame\n",
    "# 使用UDF对raw_content字段进行处理\n",
    "split_result = spark_df.withColumn(\"split_content\", split_udf(spark_df[\"raw_content\"]))\n",
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(split_result.summary())\n",
    "    split_result.select(\"url\",\"length\",\"nlines\",\"raw_content\",\"split_content\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd212cd6-25ca-434a-b633-01ca1e7b7ecd",
   "metadata": {},
   "source": [
    "### 4.3 将新字段展开获取paragraph级别row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70f305dc-6e04-40a9-8138-d3049dc4c070",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 17:40:35 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "DataFrame[summary: string, url: string, date_download: string, digest: string, length: string, nlines: string, source_domain: string, title: string, raw_content: string, raw_line_id: string, raw_line: string]\n",
      "+--------------------+--------------------+-----------+------------------------+\n",
      "|                 url|         raw_content|raw_line_id|                raw_line|\n",
      "+--------------------+--------------------+-----------+------------------------+\n",
      "|https://www.telel...|English\\tEnglish\\...|          0|    English\\tEnglish\\ten|\n",
      "|https://www.telel...|English\\tEnglish\\...|          1|繁體中文\\tChinese (Tr...|\n",
      "|https://www.telel...|English\\tEnglish\\...|          2|                    Home|\n",
      "|https://www.telel...|English\\tEnglish\\...|          3|                Products|\n",
      "|https://www.telel...|English\\tEnglish\\...|          4|              Digital TV|\n",
      "+--------------------+--------------------+-----------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:1.0045967102050781s\n"
     ]
    }
   ],
   "source": [
    "# Explode the split_content column and select the desired columns\n",
    "exploded_df = split_result.select(\"url\",\"date_download\",\"digest\",\"length\",\"nlines\",\"source_domain\",\"title\",\"raw_content\", explode(split_result.split_content).alias(\"exploded_content\"))\n",
    "\n",
    "# Split the exploded_content struct into separate columns\n",
    "exploded_df = exploded_df.withColumn(\"raw_line_id\", exploded_df.exploded_content.raw_line_id)\n",
    "exploded_df = exploded_df.withColumn(\"raw_line\", exploded_df.exploded_content.raw_line)\n",
    "\n",
    "# Drop the exploded_content column if needed\n",
    "exploded_df = exploded_df.drop(\"exploded_content\")\n",
    "\n",
    "if(mode_para[\"isTest\"]):\n",
    "    exploded_df.cache()\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(exploded_df.summary())\n",
    "    exploded_df.select(\"url\",\"raw_content\",\"raw_line_id\",\"raw_line\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a82cec-45d4-4121-b570-c38091c425fd",
   "metadata": {},
   "source": [
    "### 4.4 添加hash 列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31d74e40-0f12-4556-bac2-0bd91c94cada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASH_SIZE:8\n",
      "=== TestMode Log:\n",
      "DataFrame[summary: string, url: string, date_download: string, digest: string, length: string, nlines: string, source_domain: string, title: string, raw_content: string, raw_line_id: string, raw_line: string]\n",
      "+--------------------+--------------------+--------------------+------+------+----------------+--------------------+--------------------+-----------+------------------------+--------------------+\n",
      "|                 url|       date_download|              digest|length|nlines|   source_domain|               title|         raw_content|raw_line_id|                raw_line|          hash_value|\n",
      "+--------------------+--------------------+--------------------+------+------+----------------+--------------------+--------------------+-----------+------------------------+--------------------+\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4669|   111|www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|          0|    English\\tEnglish\\ten|[A7 E1 3C F2 70 F...|\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4669|   111|www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|          1|繁體中文\\tChinese (Tr...|[3E DB 9E EF B5 2...|\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4669|   111|www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|          2|                    Home|[E8 32 49 BD 3B A...|\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4669|   111|www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|          3|                Products|[FB DC 4F 23 F9 3...|\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4669|   111|www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|          4|              Digital TV|[59 44 27 AB 00 F...|\n",
      "+--------------------+--------------------+--------------------+------+------+----------------+--------------------+--------------------+-----------+------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:0.4633481502532959s\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BinaryType\n",
    "from ccnet_spark import normalize_for_dedup\n",
    "from typing import Iterable, Iterator, Sequence, Sized, Tuple, Type\n",
    "HASH_TYPE: Type[np.uint64] = np.uint64\n",
    "HASH_SIZE = HASH_TYPE(0).nbytes \n",
    "print(f\"HASH_SIZE:{HASH_SIZE}\") # 8 Byte ==> 64bit\n",
    "@udf(returnType=BinaryType())\n",
    "def compute_hashes(line):\n",
    "    if not line:\n",
    "        return None\n",
    "    normalized_line = normalize_for_dedup(line)  # Assuming normalize_for_dedup is defined\n",
    "    line_hash = hashlib.sha1(bytes(normalized_line, encoding=\"utf-8\")).digest()[:HASH_SIZE]\n",
    "    return line_hash\n",
    "\n",
    "# Assuming you have a dataframe named 'df' with a 'raw_line' column\n",
    "hash_df = exploded_df.withColumn(\"hash_value\", compute_hashes(exploded_df.raw_line))\n",
    "\n",
    "# Show the resulting dataframe\n",
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(hash_df.summary())\n",
    "    hash_df.show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0d5a4-2cd6-4925-8f53-690b0213bf9c",
   "metadata": {},
   "source": [
    "### 4.5根据 hash 去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "620aaf6b-9774-4a6f-98a2-962db1f9a9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "DataFrame[summary: string, url: string, date_download: string, digest: string, length: string, nlines: string, source_domain: string, title: string, raw_content: string, raw_line_id: string, raw_line: string]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 17:40:37 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n",
      "24/04/01 17:40:37 WARN TaskSetManager: Stage 7 contains a task of very large size (1316 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+------------------------------+-----------+--------------------+\n",
      "|                 url|length|nlines|                   raw_content|raw_line_id|          hash_value|\n",
      "+--------------------+------+------+------------------------------+-----------+--------------------+\n",
      "|http://www.region...|  5765|   213|          Salta al contenut...|         22|[00 00 B7 6B E5 F...|\n",
      "|http://www.darulh...| 28049|   839|          Slå på/av meny\\nD...|        738|[00 02 2B BA 78 8...|\n",
      "|http://outdoormag...|  5695|   156|          O nas\\nRedakcja\\n...|        128|[00 02 FD 23 BA F...|\n",
      "|http://stk1031.bl...|  4265|   246|きまぐれあれやこれ\\n2018年5...|        167|[00 06 62 88 AC 5...|\n",
      "|https://jakandjil...|  3258|   305|          A moda está em tu...|         21|[00 08 83 89 3F 2...|\n",
      "+--------------------+------+------+------------------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:35.5981240272522s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "deduplicated_df = hash_df.dropDuplicates(['hash_value'])\n",
    "# Show the resulting dataframe\n",
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    deduplicated_df.cache()\n",
    "    s=time.time()\n",
    "    print(deduplicated_df.summary())\n",
    "    deduplicated_df.select(\"url\",\"length\",\"nlines\",\"raw_content\",\"raw_line_id\",\"hash_value\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ed5d3-f4b3-4d94-a663-6c9279071b5d",
   "metadata": {},
   "source": [
    "### 4.6 聚合\n",
    "将段落重新聚合为doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "48e9c8fc-fdd2-4d71-81ae-77b6ebf77805",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 17:41:13 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 17:41:21 WARN MemoryStore: Not enough space to cache rdd_124_7 in memory! (computed 670.4 MiB so far)\n",
      "24/04/01 17:41:21 WARN MemoryStore: Not enough space to cache rdd_124_5 in memory! (computed 681.7 MiB so far)\n",
      "24/04/01 17:41:21 WARN BlockManager: Persisting block rdd_124_5 to disk instead.\n",
      "24/04/01 17:41:21 WARN BlockManager: Persisting block rdd_124_7 to disk instead.\n",
      "24/04/01 17:41:21 WARN MemoryStore: Not enough space to cache rdd_124_9 in memory! (computed 674.1 MiB so far)\n",
      "24/04/01 17:41:21 WARN BlockManager: Persisting block rdd_124_9 to disk instead.\n",
      "[Stage 10:==========================================>              (9 + 3) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+---------------+---------------------------+------+------+\n",
      "|                 url|original_length|original_nlines|                raw_content|length|nlines|\n",
      "+--------------------+---------------+---------------+---------------------------+------+------+\n",
      "|http://mylandia.r...|           1671|             88|       Отдушка Лунный цв...|  1143|    42|\n",
      "|http://www.recenz...|           4757|             46|       Tym, co niesamowi...|  4600|    35|\n",
      "|http://kuraruk.ha...|           1435|            100|9月の旅：東京編\\nKYOTO (...|  1062|    66|\n",
      "|https://www.govtj...|          15891|            386|       AP Fire Departmen...| 14328|   316|\n",
      "|http://www.millyb...|          23563|           1007|       View Shopping Car...| 15909|   250|\n",
      "+--------------------+---------------+---------------+---------------------------+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:11.720622539520264s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\"url\",\"date_download\",\"digest\",\"length\",\"nlines\",\"source_domain\",\"title\",\"raw_content\",\n",
    "group_df = deduplicated_df.groupBy(\"digest\").agg(\n",
    "    F.first(\"url\").alias(\"url\"),\n",
    "    F.first(\"date_download\").alias(\"date_download\"),\n",
    "    F.first(\"source_domain\").alias(\"source_domain\"),\n",
    "    F.first(\"length\").alias(\"original_length\"),\n",
    "    F.first(\"nlines\").alias(\"original_nlines\"),\n",
    "    F.first(\"title\").alias(\"title\"),\n",
    "    F.concat_ws(\"\\n\", F.collect_list(\"raw_line\").alias(\"raw_content\")).alias(\"raw_content\"),\n",
    "    F.count(\"raw_line_id\").alias(\"nlines\"),\n",
    "    F.collect_list(\"raw_line_id\").alias(\"line_ids\"),\n",
    ")\n",
    "group_df=group_df.withColumn(\"length\", F.length(group_df[\"raw_content\"]))\n",
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    group_df.cache()\n",
    "    s=time.time()\n",
    "    group_df.select(\"url\",\"original_length\",\"original_nlines\",\"raw_content\",\"length\",\"nlines\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59366d-6d0e-4ab8-937b-1dbcb39a1304",
   "metadata": {},
   "source": [
    "### 4.7 计算留存比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1295a70-8dce-40c1-aad2-103efb493626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "origin chars:30.588446M,remain_chars:24.302366999999997M \n",
      "             keep chars:79.449 % time consume:0.6132962703704834\n"
     ]
    }
   ],
   "source": [
    "if mode_para[\"isTest\"]:\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    origin_chars = spark_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    remain_chars = group_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    e = time.time()\n",
    "    print(f\"origin chars:{origin_chars/1000/1000}M,remain_chars:{remain_chars/1000/1000}M \\n \\\n",
    "            keep chars:{round(remain_chars/origin_chars*100,3)} % time consume:{e-s}\")\n",
    "else:\n",
    "    print(\"=== DevMode Log:\")\n",
    "    s = time.time()\n",
    "    origin_chars = spark_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    remain_chars = group_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    e = time.time()\n",
    "    print(f\"origin chars:{origin_chars/1000/1000}M,remain_chars:{remain_chars/1000/1000}M \\n \\\n",
    "            keep chars:{round(remain_chars/origin_chars*100,3)} % time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1063646-dc67-44d2-acfd-80be5ba32b88",
   "metadata": {},
   "source": [
    "## 5. 语言识别导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42a4f573-245e-483b-a9d8-d8c79f2097fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "[Stage 24:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------+----+-----+\n",
      "|                 url|                raw_content|lang|score|\n",
      "+--------------------+---------------------------+----+-----+\n",
      "|http://mylandia.r...|       Отдушка Лунный цв...|  ru| 0.98|\n",
      "|http://www.recenz...|       Tym, co niesamowi...|  pl|  1.0|\n",
      "|http://kuraruk.ha...|9月の旅：東京編\\nKYOTO (...|  ja|  1.0|\n",
      "|https://www.govtj...|       AP Fire Departmen...|  en| 0.73|\n",
      "|http://www.millyb...|       View Shopping Car...|  en| 0.59|\n",
      "+--------------------+---------------------------+----+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:3.5569612979888916s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, FloatType\n",
    "\n",
    "import fasttext  # type: ignore\n",
    "\n",
    "@cached(cache={})\n",
    "def getFastTextModel():\n",
    "    model_path = \"models/fasttext/lid.bin\"\n",
    "    fasttext_model = fasttext.load_model(model_path)\n",
    "    return fasttext_model\n",
    "def predict(model, text: str, k: int = 1):\n",
    "    labels, scores = model.predict(text, k=k)\n",
    "    labels = [l.replace(\"__label__\", \"\") for l in labels]\n",
    "    return labels, scores\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def predictLang(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    labels, scores = predict(getFastTextModel(), text.replace(\"\\n\", \"\"), k=1)\n",
    "    scores.round(2, out=scores)\n",
    "    lang = labels[0]\n",
    "    score = scores[0]\n",
    "    if score < 0.5:\n",
    "        return None\n",
    "    return lang\n",
    "@udf(returnType=FloatType())\n",
    "def predictScore(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    labels, scores = predict(getFastTextModel(), text.replace(\"\\n\", \"\"), k=1)\n",
    "    scores.round(2, out=scores)\n",
    "    lang = labels[0]\n",
    "    score = scores[0]\n",
    "    if score < 0.5:\n",
    "        return None\n",
    "    return float(score)\n",
    "lang_df = group_df.withColumn(\"lang\", predictLang(\"raw_content\"))\n",
    "lang_df = lang_df.withColumn(\"score\", predictScore(\"raw_content\"))\n",
    "\n",
    "if mode_para[\"isTest\"]:\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    lang_df.select(\"url\",\"raw_content\",\"lang\",\"score\").show(5)\n",
    "    e = time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875128c6-b78c-4ffb-b9b6-aa5fc13e0220",
   "metadata": {},
   "source": [
    "## 6. MultiSentencePiece 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fbc282bb-2819-4caa-bc51-f357b8d67b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Iterable, List, NamedTuple, Optional, Sequence, Tuple\n",
    "import sentencepiece  # type: ignore\n",
    "lm_dir: Path = Path(\"../cc_net/data/lm_sp\")\n",
    "\n",
    "def get_lm_languages() -> Sequence[str]:\n",
    "    languages = [m.name.split(\".\")[0] for m in lm_dir.glob(\"*.arpa.bin\")]\n",
    "    return languages\n",
    "\n",
    "@cached(cache={})\n",
    "def getLMModel(lang):\n",
    "    models={l: lm_dir / f\"{l}.sp.model\" for l in get_lm_languages()}\n",
    "    lms=get_lm_languages()\n",
    "    if(lms is None or lang not in lms):\n",
    "        return None\n",
    "    sp = sentencepiece.SentencePieceProcessor()\n",
    "    sp.load(str(models[lang]))\n",
    "    return sp\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def doSentencePiece(text,lang):\n",
    "    if text is None or lang is None:\n",
    "        return None\n",
    "    text = text_normalizer.normalize(text)\n",
    "    sp = getLMModel(lang)\n",
    "    if sp is None:\n",
    "        return None\n",
    "    tokenized = sp.encode_as_pieces(text)\n",
    "    return \" \".join(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dd953788-afe5-4b0a-9240-b703612eefe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------+----+-----+--------------------------+\n",
      "|                 url|                raw_content|lang|score|                 tokenized|\n",
      "+--------------------+---------------------------+----+-----+--------------------------+\n",
      "|http://mylandia.r...|       Отдушка Лунный цв...|  ru| 0.98|      ▁от душ ка ▁лунны...|\n",
      "|http://www.recenz...|       Tym, co niesamowi...|  pl|  1.0|      ▁tym , ▁co ▁nie s...|\n",
      "|http://kuraruk.ha...|9月の旅：東京編\\nKYOTO (...|  ja|  1.0|▁ 0 月 の旅 : 東京 編 k...|\n",
      "|https://www.govtj...|       AP Fire Departmen...|  en| 0.73|      ▁ap ▁fire ▁depart...|\n",
      "|http://www.millyb...|       View Shopping Car...|  en| 0.59|      ▁view ▁shopping ▁...|\n",
      "+--------------------+---------------------------+----+-----+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:0.6076529026031494s\n"
     ]
    }
   ],
   "source": [
    "lm_df = lang_df.withColumn(\"tokenized\", doSentencePiece(\"raw_content\",\"lang\"))\n",
    "if mode_para[\"isTest\"]:\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    lm_df.select(\"url\",\"raw_content\",\"lang\",\"score\",\"tokenized\").show(5)\n",
    "    e = time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad98f5-b0ab-4dea-a631-d12920e92e6b",
   "metadata": {},
   "source": [
    "## 7. 困惑度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3050abb1-6b04-4473-858a-276abd31f3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dir: Path = Path(\"../cc_net/data/lm_sp\")\n",
    "import kenlm  # type: ignore\n",
    "\n",
    "@cached(cache={})\n",
    "def getDocLMModel(lang):\n",
    "    models={l: lm_dir / f\"{l}.arpa.bin\" for l in get_lm_languages()}\n",
    "    lms=get_lm_languages()\n",
    "    if(lms is None or lang not in lms):\n",
    "        return None\n",
    "    lm_config = kenlm.Config()\n",
    "    lm_config.load_method = 2\n",
    "    lm = kenlm.Model(str(models[lang]), lm_config)\n",
    "    return lm\n",
    "def pp(log_score, length):\n",
    "    return 10.0 ** (-log_score / length)\n",
    "@udf(returnType=FloatType())\n",
    "def doDocLM(text,lang):\n",
    "    if text is None or lang is None:\n",
    "        return None\n",
    "    model = getDocLMModel(lang)\n",
    "    if model is None:\n",
    "        return None\n",
    "    lines = text.split(\"\\n\")\n",
    "\n",
    "    doc_log_score, doc_length = 0, 0\n",
    "    for line in lines:\n",
    "        log_score = model.score(line)\n",
    "        length = len(line.split()) + 1\n",
    "        doc_log_score += log_score\n",
    "        doc_length += length\n",
    "    return round(pp(doc_log_score, doc_length), 1)\n",
    "doclm=getDocLMModel(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73420a13-2620-48e5-8ee4-f2db7747d75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------+----+-----+--------------------------+----------+\n",
      "|                 url|                raw_content|lang|score|                 tokenized|perplexity|\n",
      "+--------------------+---------------------------+----+-----+--------------------------+----------+\n",
      "|http://mylandia.r...|       Отдушка Лунный цв...|  ru| 0.98|      ▁от душ ка ▁лунны...|     719.4|\n",
      "|http://www.recenz...|       Tym, co niesamowi...|  pl|  1.0|      ▁tym , ▁co ▁nie s...|     238.7|\n",
      "|http://kuraruk.ha...|9月の旅：東京編\\nKYOTO (...|  ja|  1.0|▁ 0 月 の旅 : 東京 編 k...|    1426.7|\n",
      "|https://www.govtj...|       AP Fire Departmen...|  en| 0.73|      ▁ap ▁fire ▁depart...|    3504.9|\n",
      "|http://www.millyb...|       View Shopping Car...|  en| 0.59|      ▁view ▁shopping ▁...|    1817.6|\n",
      "+--------------------+---------------------------+----+-----+--------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:0.940995454788208s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "doclm_df = lm_df.withColumn(\"perplexity\", doDocLM(\"tokenized\",\"lang\"))\n",
    "if mode_para[\"isTest\"]:\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    doclm_df.select(\"url\",\"raw_content\",\"lang\",\"score\",\"tokenized\",\"perplexity\").show(5)\n",
    "    e = time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629b727-1d96-4f15-95cd-897a5ff37df7",
   "metadata": {},
   "source": [
    "## 8. PerplexityBucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b34556e7-0299-45f1-9f86-818e674c42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff_csv = \"../cc_net/cc_net/\" + \"data/\" + \"cutoff.csv\"\n",
    "percentile_head: int = 30\n",
    "percentile_tail: int = 60\n",
    "cutoffs = pd.read_csv(cutoff_csv, index_col=0)\n",
    "cutoffs = {\n",
    "    l: (cutoffs[l][percentile_head], cutoffs[l][percentile_tail])\n",
    "    for l in cutoffs.columns\n",
    "}\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def doPPBucket(perplexity,lang):\n",
    "    if (perplexity is None):\n",
    "        perplexity = -1\n",
    "    if lang not in cutoffs or perplexity < 0:\n",
    "        return \"all\"\n",
    "    pp_head, pp_tail = cutoffs[lang]\n",
    "    if perplexity < pp_head:\n",
    "        return \"head\"\n",
    "    if perplexity < pp_tail:\n",
    "        return \"middle\"\n",
    "    return \"tail\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3740bb02-7c9c-4931-9d94-8a4ff2da3eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "[Stage 33:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------------------------+----+-----+------------------------------+----------+------+\n",
      "|                 url|                          raw_content|lang|score|                     tokenized|perplexity|bucket|\n",
      "+--------------------+-------------------------------------+----+-----+------------------------------+----------+------+\n",
      "|http://mylandia.r...|                 Отдушка Лунный цв...|  ru| 0.98|          ▁от душ ка ▁лунны...|     719.4|  tail|\n",
      "|http://www.recenz...|                 Tym, co niesamowi...|  pl|  1.0|          ▁tym , ▁co ▁nie s...|     238.7|middle|\n",
      "|http://kuraruk.ha...|          9月の旅：東京編\\nKYOTO (...|  ja|  1.0|    ▁ 0 月 の旅 : 東京 編 k...|    1426.7|middle|\n",
      "|https://www.govtj...|                 AP Fire Departmen...|  en| 0.73|          ▁ap ▁fire ▁depart...|    3504.9|  tail|\n",
      "|http://www.millyb...|                 View Shopping Car...|  en| 0.59|          ▁view ▁shopping ▁...|    1817.6|  tail|\n",
      "|https://www.notti...|                 Nottingham Rugby ...|  en| 0.87|          ▁nottingham ▁rugb...|     703.6|  tail|\n",
      "|http://www.revizo...|                 Događanja\\n«\\t ve...|null| null|                          null|      null|   all|\n",
      "|https://www.kupre...|                 26. 8. 2015\\nNásl...|  cs| 0.98|          ▁00. ▁0. ▁0000 na...|    3020.9|  tail|\n",
      "|http://internatra...|                 Родители: мать-од...|  ru| 0.99|          ▁родители : ▁мать...|     311.1|  tail|\n",
      "|http://www.xda.cn...|旋转电子为未来的混合电子设备打开了...|  zh| 0.96|▁ 旋转 电子 为 未来的 混合 ...|    2308.5|  tail|\n",
      "|https://www.indus...|                 SS8, Inc. (Milpit...|  en| 0.74|          ▁s s 0 , ▁inc . ▁...|    3165.0|  tail|\n",
      "|https://www.fight...|                 Slip & Fall\\nMoto...|  en| 0.81|          ▁slip ▁ & ▁fall m...|    1158.1|  tail|\n",
      "|http://forum.blac...|                 Posted By: Purple...|  en| 0.96|          ▁posted ▁by : ▁pu...|     489.6|middle|\n",
      "|http://ballherehe...|        02.03 특별기획드라마 바벨....|  ru| 0.52|   ▁00.00 ▁ 특별기획드라마 ...|     674.6|  tail|\n",
      "|https://sweetsoul...|                 RELEASE\\nArtist:\\...|null| null|                          null|      null|   all|\n",
      "|http://semestr.co...|                 под ред. Я.К. Шми...|  ru| 0.96|          ▁под ▁ред . ▁ я ....|     873.1|  tail|\n",
      "|https://www.alfae...|                 Categoria ragazzi...|  it| 0.85|          ▁categoria ▁ragaz...|    1160.6|  tail|\n",
      "|https://www.cappe...|                 Sports Handicappi...|  en| 0.73|          ▁sports ▁handicap...|    2845.0|  tail|\n",
      "|http://www.af-out...|           adidas 倉石一樹\\n卡麗\\n...|null| null|                          null|      null|   all|\n",
      "|https://www.bitst...|    条款与规则\\n欢迎礼包\\n隐私政策...|  zh|  0.9| ▁ 条款 与 规则 欢迎 礼 包 ...|    7623.6|  tail|\n",
      "|http://www.softem...|                 Email software li...|  en| 0.75|          ▁email ▁software ...|    1975.5|  tail|\n",
      "|http://melodicsou...|                 Hersteller: AFM\\n...|  de|  0.5|          ▁hersteller : ▁af...|    1963.1|  tail|\n",
      "|http://dziatwa.co...|                 — TwojaStara\\n7 4...|  pl| 0.94|          ▁- ▁two ja star a...|     898.8|  tail|\n",
      "|http://okconferen...|                 Cedar Springs\\nGi...|  en| 0.74|          ▁cedar ▁springs g...|    2261.3|  tail|\n",
      "|http://www.daycar...|                 Opening a Daycare...|  en| 0.84|          ▁opening ▁a ▁day ...|    1164.9|  tail|\n",
      "|https://www.bienp...|                 Questions – Répon...|  fr| 0.98|          ▁questions ▁- ▁re...|     246.3|middle|\n",
      "|https://californi...|                 Providence Medica...|  en| 0.77|          ▁providence ▁medi...|    1553.4|  tail|\n",
      "|https://www.deeja...|                 Kopfhörer / Headp...|  en| 0.66|          ▁ kopf hor er ▁/ ...|    1887.4|  tail|\n",
      "|http://www.hb11x5...|      金价周四收高1%创两周新高2011...|  zh| 0.83|   ▁金 价 周 四 收 高 0% 创...|    1629.4|middle|\n",
      "|https://wiadomosc...|                 Politycy:\\nWszyst...|  pl|  1.0|          ▁polityc y : wszy...|     545.9|  tail|\n",
      "|https://pumpsandt...|                 -- Default Style ...|  en| 0.74|          ▁-- ▁default ▁sty...|    1874.4|  tail|\n",
      "|https://www.brech...|                 Jouw account\\nNie...|  nl| 0.92|          ▁jouw ▁account ni...|    3794.4|  tail|\n",
      "|https://cyrrusgal...|                 Berlin,Germany.\\n...|  en| 0.94|          ▁berlin , german ...|     536.0|  tail|\n",
      "|http://ciacenter....|                 If you have probl...|  en| 0.77|          ▁if ▁you ▁have ▁p...|     646.5|  tail|\n",
      "|https://uhti-tuht...|                 Санки37\\nМебель д...|  ru| 0.99|          ▁сан ки 00 м е бе...|     634.7|  tail|\n",
      "|http://www.naturf...|                 Webkamera Makov\\n...|  cs| 0.97|          ▁web ka mera ▁ma ...|    2002.5|  tail|\n",
      "|http://fifaplanet...|                 AW: Revolution Da...|  de| 0.99|          ▁aw : ▁revolution...|     806.1|  tail|\n",
      "|https://peachcoun...|                 Incentives\\nFreep...|  en| 0.85|          ▁incentive s free...|     730.2|  tail|\n",
      "|http://www.urocni...|                 Válečné a pováleč...|  cs| 0.99|          ▁valecne ▁a ▁pova...|    3126.6|  tail|\n",
      "|http://penstewart...|                 Kenmerkend zijn d...|  nl| 0.98|          ▁kenmerkend ▁zijn...|     637.4|middle|\n",
      "|http://bgames.big...|                 T&Cs\\n2019 COMPET...|  en| 0.95|          ▁t & c s 0000 ▁co...|     376.0|middle|\n",
      "|https://acervaes....|                 Concurso 2018\\nSe...|  pt| 0.94|          ▁concurso ▁0000 s...|     554.6|  tail|\n",
      "|https://walkiride...|                 Actualizaciones d...|  es| 0.86|          ▁actualizacion es...|     554.1|  tail|\n",
      "|http://saint-enne...|                 Danse Country\\nLe...|  fr| 0.95|          ▁dans e ▁country ...|    1379.6|  tail|\n",
      "|https://www.gootv...|               【MV】Kimi wa Melod...|  th| 0.51|                          null|      null|   all|\n",
      "|http://teleguide....|                 Итоги матча Росси...|  ru|  1.0|          ▁итог и ▁матча ▁р...|     268.3|middle|\n",
      "|https://www.afish...|                 Благовещенск\\nНиж...|  ru|  1.0|          ▁благовещенск ни ...|     363.9|  tail|\n",
      "|https://www.lifes...|                 LifeStream Servic...|  en| 0.89|          ▁life stream ▁ser...|     545.3|  tail|\n",
      "|http://enbohms.se...|                 Webbplatsen uppda...|  sv| 0.88|                          null|      null|   all|\n",
      "|https://www.hgexp...|                 MIT- Advanced Dat...|  en|  0.9|          ▁mit - ▁advanced ...|     542.2|  tail|\n",
      "+--------------------+-------------------------------------+----+-----+------------------------------+----------+------+\n",
      "only showing top 50 rows\n",
      "\n",
      "time consume:43.040971517562866s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "bucket_df = doclm_df.withColumn(\"bucket\", doPPBucket(\"perplexity\",\"lang\"))\n",
    "if mode_para[\"isTest\"]:\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    bucket_df.select(\"url\",\"raw_content\",\"lang\",\"score\",\"tokenized\",\"perplexity\",\"bucket\").show(50)\n",
    "    e = time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dc2c90-c41d-4c42-a893-0131f7537b3c",
   "metadata": {},
   "source": [
    "## 9. dropKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "59a1a31e-7e6f-460e-95bd-31580e7bc324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "DataFrame[summary: string, digest: string, url: string, date_download: string, source_domain: string, original_length: string, original_nlines: string, title: string, raw_content: string, nlines: string, length: string, lang: string, score: string, perplexity: string, bucket: string]\n",
      "time consume:0.04165530204772949s\n"
     ]
    }
   ],
   "source": [
    "drop_df = bucket_df.drop(\"tokenized\")\n",
    "if mode_para[\"isTest\"]:\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    print(drop_df.summary())\n",
    "    e = time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12648326-55ec-4cb4-80d3-abffa04bb16b",
   "metadata": {},
   "source": [
    "## 10. split by lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8e58f31-8afd-40e8-8803-540a4e391035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----+------+\n",
      "|                 url|         raw_content|lang|bucket|\n",
      "+--------------------+--------------------+----+------+\n",
      "|https://www.stbal...|Grant Types\\nThe ...|  en|  head|\n",
      "|http://resourcere...|About Disease Inf...|  en|  head|\n",
      "|https://gusto.com...|1. These R&D Term...|  en|  head|\n",
      "|https://myownhome...|Reset Filter\\nCal...|  en|  head|\n",
      "|https://www.cultu...|Tagged with: #Mua...|  en|  head|\n",
      "|https://wanttokno...|How Many Countrie...|  en|  head|\n",
      "|http://www.honeys...|Changing World\\nP...|  en|  head|\n",
      "|http://www.kraken...|The Truth About t...|  en|  head|\n",
      "|https://livinwith...|bandera endurance...|  en|  head|\n",
      "|https://www.homet...|“We removed the s...|  en|  head|\n",
      "|http://boxerlove....|Boxer early genea...|  en|  head|\n",
      "|https://www.odesd...|ODES INDUSTRIES m...|  en|  head|\n",
      "|https://www.bizka...|© Bizkaia:talent\\...|  en|  head|\n",
      "|http://www.nature...|Serious Themes\\nT...|  en|  head|\n",
      "|https://clbfundy....|SPARKS OF INSPIRA...|  en|  head|\n",
      "|https://coadb.com...|References [ + ]\\...|  en|  head|\n",
      "|http://chupas2.tr...|I wish you love m...|  en|  head|\n",
      "|http://store.lest...|Only 1 of this ra...|  en|  head|\n",
      "|https://www.empty...|Half-Past Kissin'...|  en|  head|\n",
      "|https://www.kansa...|This week in Mizz...|  en|  head|\n",
      "|https://plus33pho...|by kazu\\ton May 2...|  en|  head|\n",
      "|http://s05.flagco...|Carbon dioxide em...|  en|  head|\n",
      "|http://www.drpain...|It is common to f...|  en|  head|\n",
      "|https://johandalg...|What a tour this ...|  en|  head|\n",
      "|http://gi-gi.net/...|Einstürzende Neub...|  en|  head|\n",
      "|https://pamshiple...|No discrimination...|  en|  head|\n",
      "|https://www.lewis...|1 of 15 next\\nPro...|  en|  head|\n",
      "|http://dbsjeyaraj...|Certainly the Nor...|  en|  head|\n",
      "|https://ttd.duck....|1st Free for All ...|  en|  head|\n",
      "|http://kzsipp.geo...|Webmaster: andy\\n...|  en|  head|\n",
      "|http://methodstom...|Set up the site. ...|  en|  head|\n",
      "|https://forum.the...|I guess I didn't ...|  en|  head|\n",
      "|https://blog.educ...|Pingback: 2018 Ge...|  en|  head|\n",
      "|http://wap.everto...|This is the defau...|  en|  head|\n",
      "|https://thissideo...|Dramacool\\nCopyri...|  en|  head|\n",
      "|http://search-pav...|Proudly powered b...|  en|  head|\n",
      "|http://cursodease...|That’s not how it...|  en|  head|\n",
      "|https://www.busin...|August 12, 2017 1...|  en|  head|\n",
      "|http://powerforus...|Donn’s Articles\\n...|  en|  head|\n",
      "|http://focoprendi...|Computers and Tec...|  en|  head|\n",
      "|https://myafribus...|What are your tho...|  en|  head|\n",
      "|http://matthew-le...|“I guess I wanted...|  en|  head|\n",
      "|https://drrohitva...|According to a Ce...|  en|  head|\n",
      "|http://www.ponyta...|Apple image court...|  en|  head|\n",
      "|https://supreme.j...|US Federal Law\\nJ...|  en|  head|\n",
      "|https://en.m.wiki...|Nucleotide sugars...|  en|  head|\n",
      "|https://hairandde...|[10] txtx is to d...|  en|  head|\n",
      "|http://championev...|December 16 @ 4:0...|  en|  head|\n",
      "|http://www.henryh...|Creatures (500)\\n...|  en|  head|\n",
      "|http://psychodriv...|Polar (2019)\\nI f...|  en|  head|\n",
      "+--------------------+--------------------+----+------+\n",
      "only showing top 50 rows\n",
      "\n",
      "time consume:5.523314952850342s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "if mode_para[\"isTest\"]:\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    selected_df = drop_df.filter((drop_df.lang == \"en\") & (drop_df.bucket == \"head\"))\n",
    "    selected_df.select(\"url\",\"raw_content\",\"lang\",\"bucket\").show(50)\n",
    "    e = time.time()\n",
    "    print(f\"time consume:{e-s}s\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
