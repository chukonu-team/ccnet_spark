{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28e7aa31-b893-45a3-aa85-450f056caf1f",
   "metadata": {},
   "source": [
    "# 1. 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad5b292-4eb3-4cc0-8837-2a8cf03fd7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/29 11:43:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/29 11:43:03 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from ccnet_spark import open_read, parse_warc_file,compute_hashes,NaiveHashSet\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StringType,IntegerType,StructType, StructField\n",
    "from pyspark.sql.functions import udf, explode\n",
    "\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"CCNETSpark\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99a057-a8db-4d94-b772-988434ac58cc",
   "metadata": {},
   "source": [
    "# 2. 读取文件数据，处理成pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493e968-ee52-488d-bab9-5c54cff63dd6",
   "metadata": {},
   "source": [
    "## 2.1 获取cache文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ceb17ca6-b75b-4307-8083-d5e48e5cd768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/zz/github/cache_data/2019-09/CC-MAIN-20190215183319-20190215205319-00003.warc.wet.gz\n"
     ]
    }
   ],
   "source": [
    "cache_data=\"/Users/zz/github/cache_data/2019-09/\"\n",
    "def getWETURL(segment: int):\n",
    "    cache_file_prefix = \"CC-MAIN-20190215183319-20190215205319-\"\n",
    "    cache_file_sufix = \".warc.wet.gz\"\n",
    "    segment_str = str(segment).zfill(5)  # Pad with leading zeros\n",
    "    return cache_data+cache_file_prefix + segment_str + cache_file_sufix\n",
    "url = getWETURL(3)\n",
    "print(url)  # Output: CC-MAIN-20190215183319-20190215205319-00003.warc.wet.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0530fda-e712-49b4-bab1-d9c76972b465",
   "metadata": {},
   "source": [
    "## 2.2 处理文件，存入pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aa98aa6-e289-4f48-ac9e-4810088f6b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-29 11:44 INFO 61632:ccnet_spark.load - Opening /Users/zz/github/cache_data/2019-09/CC-MAIN-20190215183319-20190215205319-00000.warc.wet.gz with mode 'rt'\n",
      "2024-03-29 11:44 INFO 61632:root - Created DataFrame with 43855 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parse one seg to pd_df consume:3.609127998352051 s\n"
     ]
    }
   ],
   "source": [
    "file_path=Path(getWETURL(0))\n",
    "file=open_read(file_path)\n",
    "s=time.time()\n",
    "pandas_df = parse_warc_file(file, 30)\n",
    "\n",
    "random_save_n=5\n",
    "pandas_df = pandas_df.sample(n=random_save_n, random_state=1)\n",
    "e=time.time()\n",
    "print(f\"parse one seg to pd_df consume:{e-s} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef42772d-a0cd-44e7-b197-312deb3ef263",
   "metadata": {},
   "source": [
    "# 3. 将pandas DataFrame 转换成spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12719579-006a-4e30-bdad-a11cd62340b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+------+------+--------------------+--------------------+--------------------+\n",
      "|                 url|       date_download|              digest|length|nlines|       source_domain|               title|         raw_content|\n",
      "+--------------------+--------------------+--------------------+------+------+--------------------+--------------------+--------------------+\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4758|   111|    www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|\n",
      "|http://www.ma.hu/...|2019-02-15T19:26:14Z|sha1:FA3DLWLJZKFI...|  4180|    70|           www.ma.hu|Kiégett a Hűvösvö...|hirdetés\\nma.hu n...|\n",
      "|http://angagement...|2019-02-15T18:57:03Z|sha1:EMAN4TLHXTXM...|  1326|    65|   angagement.com.ua|Ballroom and lati...|→ по-русски\\nCost...|\n",
      "|http://resistther...|2019-02-15T19:55:31Z|sha1:UWDJJTE42LC7...|   912|    23|resisttherock.uco...|Comments or Sugge...|Unwanted Resistan...|\n",
      "|http://klimadiagr...|2019-02-15T19:05:24Z|sha1:IZ4YL65XZD2U...|  1918|    51|   klimadiagramme.de|Das Klima in Karl...|Das Klima in Karl...|\n",
      "+--------------------+--------------------+--------------------+------+------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 将 pandas DataFrame 转换为 Spark DataFrame\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9793de8-008f-4aec-9dc7-0bbad27feff3",
   "metadata": {},
   "source": [
    "# 4. hash计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dea9e5-b3a8-4b50-b40c-147fd7bbecbd",
   "metadata": {},
   "source": [
    "## 4.1 定义UDF,将doc 分割成paragraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44be1b10-248f-470d-874b-3b3f4962a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，用于分割文本\n",
    "def split_raw_content(content):\n",
    "    lines = content.split('\\n')\n",
    "    line_ids = range(1, len(lines) + 1)  # 生成行号\n",
    "    return list(zip(line_ids, lines))\n",
    "\n",
    "# 注册为UDF\n",
    "split_udf = udf(split_raw_content, ArrayType(StructType([\n",
    "    StructField(\"raw_line_id\", IntegerType(), False),\n",
    "    StructField(\"raw_line\", StringType(), False)\n",
    "])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac16adaa-b1c1-4e99-8bdf-93312621e5f4",
   "metadata": {},
   "source": [
    "## 4.2 udf 处理添加新字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c505bf4-b9f4-4e1f-ad51-ad56939b28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设spark_df是您的DataFrame\n",
    "# 使用UDF对raw_content字段进行处理\n",
    "split_result = spark_df.withColumn(\"split_content\", split_udf(spark_df[\"raw_content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd212cd6-25ca-434a-b633-01ca1e7b7ecd",
   "metadata": {},
   "source": [
    "## 4.3 将新字段展开获取paragraph级别row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70f305dc-6e04-40a9-8138-d3049dc4c070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+------+----------------+--------------------+-----------+------------------------+\n",
      "|                 url|              digest|length|nlines|   source_domain|               title|raw_line_id|                raw_line|\n",
      "+--------------------+--------------------+------+------+----------------+--------------------+-----------+------------------------+\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|          1|    English\\tEnglish\\ten|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|          2|繁體中文\\tChinese (Tr...|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|          3|                    Home|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|          4|                Products|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|          5|              Digital TV|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|          6|    DVB / ISDB-T TV S...|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|          7|            AddTV System|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|          8|                     CAS|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|          9|          Card based CAS|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|         10|            Cardless CAS|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|         11|        Digital Heandend|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|         12|     ASI to IP converter|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|         13|                 Encoder|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|         14|    IP Mux / Scramble...|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|         15|                     IRD|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|         16|    Mux / Scrambler /...|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|         17|              Transcoder|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|         18|                     EPG|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|         19|         Push VOD System|\n",
      "|https://www.telel...|sha1:VZYTYZZ7EH6E...|  4758|   111|www.telelynx.com|sean, Author at T...|         20|    SMS / Billing / M...|\n",
      "+--------------------+--------------------+------+------+----------------+--------------------+-----------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode the split_content column and select the desired columns\n",
    "exploded_df = split_result.select(\"url\",\"digest\",\"length\",\"nlines\",\"source_domain\",\"title\", explode(split_result.split_content).alias(\"exploded_content\"))\n",
    "\n",
    "# Split the exploded_content struct into separate columns\n",
    "exploded_df = exploded_df.withColumn(\"raw_line_id\", exploded_df.exploded_content.raw_line_id)\n",
    "exploded_df = exploded_df.withColumn(\"raw_line\", exploded_df.exploded_content.raw_line)\n",
    "\n",
    "# Drop the exploded_content column if needed\n",
    "exploded_df = exploded_df.drop(\"exploded_content\")\n",
    "\n",
    "# Show the resulting dataframe\n",
    "exploded_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad21e97c-2fdb-456a-b5d9-a8eff46dc7e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
