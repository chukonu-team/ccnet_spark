{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e12de46-32a0-47cf-99c2-1b1150874cc0",
   "metadata": {},
   "source": [
    "# ccnet spark pandas udf pipeline 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf452cae-499d-4429-8231-7ad93bd12a97",
   "metadata": {},
   "source": [
    "## 1. 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63865c5c-5aa3-41e1-8154-ecc357d348e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 17:29:10 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/04/01 17:29:10 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from ccnet_spark import open_read, parse_warc_file,compute_hashes,NaiveHashSet, text_normalizer\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StringType,IntegerType,StructType, StructField\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"CCNETPandasSpark\")  \\\n",
    "                    .config(\"spark.executor.memory\", \"100g\") \\\n",
    "                    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "                    .config(\"spark.driver.maxResultSize\", \"32g\") \\\n",
    "                    .config('spark.sql.execution.arrow.pyspark.enabled', 'true') \\\n",
    "                    .getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a43bf3c-e37e-4d95-9747-57c50326656b",
   "metadata": {},
   "source": [
    "## 2. 读取文件数据，处理成pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b136dd03-90a1-42cb-a25f-6ceef27dc86f",
   "metadata": {},
   "source": [
    "### 2.1 获取cache文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d490ae-196b-4052-aa39-9a8610a5531b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../cache_data/2019-09/CC-MAIN-20190215183319-20190215205319-00003.warc.wet.gz\n"
     ]
    }
   ],
   "source": [
    "cache_data=\"../cache_data/2019-09/\"\n",
    "def getWETURL(segment: int):\n",
    "    cache_file_prefix = \"CC-MAIN-20190215183319-20190215205319-\"\n",
    "    cache_file_sufix = \".warc.wet.gz\"\n",
    "    segment_str = str(segment).zfill(5)  # Pad with leading zeros\n",
    "    return cache_data+cache_file_prefix + segment_str + cache_file_sufix\n",
    "url = getWETURL(3)\n",
    "print(url)  # Output: CC-MAIN-20190215183319-20190215205319-00003.warc.wet.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114a234c-f5bc-42ba-8fbc-10e5ed8aa0ff",
   "metadata": {},
   "source": [
    "### 2.2 处理文件，存入pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b150483-2dc3-4de2-baa9-ef6f9208126a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getpdf(segment,isPart:bool):\n",
    "    file_path=Path(getWETURL(segment))\n",
    "    file=open_read(file_path)\n",
    "    s=time.time()\n",
    "    pandas_df = parse_warc_file(file, 30)\n",
    "    if(isPart):\n",
    "        random_save_n=100\n",
    "        pandas_df = pandas_df.sample(n=random_save_n, random_state=1)\n",
    "    e=time.time()\n",
    "    print(f\"====== parse segment:{segment} to pd_df consume:{e-s} s\")\n",
    "    return pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ec2a20-bce0-4b57-9aff-1813c79915a6",
   "metadata": {},
   "source": [
    "## 3. 读取 spark dataframe 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1bce131-84ed-4275-af5f-2a2073479ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsdf(segment,isPart:bool):\n",
    "    inner_path = \"_part\" if isPart else \"_all\"\n",
    "    output_path = cache_data+\"cache_parquet/\"+str(segment)+  inner_path +\".parquet\"  # 设置输出路径\n",
    "    # 检查本地文件是否存在\n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"======process to parquet of segment {segment}{inner_path}\")\n",
    "        # 处理文件并生成 Spark DataFrame\n",
    "        pdf = getpdf(segment,isPart=isPart)\n",
    "        pdf.to_parquet(output_path)  # 保存为 parquet 文件\n",
    "        spark_df = spark.createDataFrame(pdf)\n",
    "    else:\n",
    "        print(f\"======read parquet of segment {segment}{inner_path} from cache\")\n",
    "        pdf = pd.read_parquet(output_path)\n",
    "        spark_df = spark.createDataFrame(pdf)\n",
    "    return spark_df\n",
    "def getsdfs(segments,isPart:bool = False):\n",
    "    merged_sdf=None\n",
    "    for seg in segments:\n",
    "        if(merged_sdf):\n",
    "            merged_sdf = merged_sdf.unionAll(getsdf(seg,isPart)) # Merge DataFrames\n",
    "        else:\n",
    "            merged_sdf = getsdf(seg,isPart)\n",
    "    return merged_sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469b7a43-2411-402e-979b-023f9189dace",
   "metadata": {},
   "source": [
    "### 3.1 load spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04fbb057-2a4b-45db-a9d9-d46e96aa4f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModePara(mode):\n",
    "    if(mode==\"test\"):\n",
    "        para={\n",
    "            \"isTest\":True,\n",
    "            \"isPart\":True,\n",
    "            \"segments\":5,\n",
    "        }\n",
    "        return para\n",
    "    else:\n",
    "        para={\n",
    "            \"isTest\":False,\n",
    "            \"isPart\":False,\n",
    "            \"segments\":40,\n",
    "        }\n",
    "        return para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "940f98da-8c17-4d13-a832-ffd2ca2cfd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode=\"test\"\n",
    "mode_para=getModePara(mode)\n",
    "segments=[i for i in range(mode_para[\"segments\"])]\n",
    "isPart=mode_para[\"isPart\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3491b3b3-5729-4ac8-b0ed-400b1ef4a27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======read parquet of segment 0_part from cache\n",
      "======read parquet of segment 1_part from cache\n",
      "======read parquet of segment 2_part from cache\n",
      "======read parquet of segment 3_part from cache\n",
      "======read parquet of segment 4_part from cache\n",
      "load 5 segments,with 5000 docs,comsume:3.961294174194336s\n"
     ]
    }
   ],
   "source": [
    "s=time.time()\n",
    "spark_df = getsdfs(segments,isPart=isPart)\n",
    "num_docs=spark_df.count()\n",
    "e=time.time()\n",
    "print(f\"load {len(segments)} segments,with {num_docs} docs,comsume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa6b54a-df5b-4820-b188-d0ab6e898a3f",
   "metadata": {},
   "source": [
    "### 3.2 字段分析\n",
    "1. wet 文件本身带有长度：\"length\": length,这个是从wet的\"Content-Length:\"读出来的，和我计算len(raw_content）有出入。考虑原因是原先的length不只是说raw_content，还包括title等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b463a837-64f8-4318-ad17-30a9b5991878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "DataFrame[summary: string, url: string, date_download: string, digest: string, length: string, nlines: string, source_domain: string, title: string, raw_content: string]\n",
      "+--------------------+------+------+--------------+\n",
      "|                 url|length|nlines|compute_length|\n",
      "+--------------------+------+------+--------------+\n",
      "|https://www.telel...|  4758|   111|          4669|\n",
      "|http://www.ma.hu/...|  4180|    70|          3716|\n",
      "|http://angagement...|  1326|    65|          1231|\n",
      "|http://resistther...|   912|    23|           868|\n",
      "|http://klimadiagr...|  1918|    51|          1851|\n",
      "+--------------------+------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:0.6141552925109863s\n"
     ]
    }
   ],
   "source": [
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(spark_df.summary())\n",
    "    tmp_df = spark_df.withColumn(\"compute_length\", F.length(spark_df[\"raw_content\"]))\n",
    "    tmp_df.select(\"url\",\"length\",\"nlines\",\"compute_length\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af3c263-73b7-4ff6-92e8-f0e830f8a52b",
   "metadata": {},
   "source": [
    "### 3.3 修改length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1325fe48-9383-47c8-9bb8-5c99bb3795f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df=spark_df.withColumn(\"length\", F.length(spark_df[\"raw_content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cbc449-a0be-4440-9f7d-d732fc832b2d",
   "metadata": {},
   "source": [
    "## 4. hash计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bda60ee-fde0-483d-b78a-dd55fe65d692",
   "metadata": {},
   "source": [
    "### 4.1 定义UDF,将doc 分割成paragraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ef1663f-b06a-41f9-bcdb-aa639f8f082f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/ccnet_spark/lib/python3.9/site-packages/pyspark/sql/pandas/functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import ArrayType, StructType, StructField, IntegerType, StringType\n",
    "\n",
    "# 定义一个函数，用于分割文本\n",
    "def split_raw_content(content):\n",
    "    lines = content.split(\"\\n\")\n",
    "    line_lengths = [ (index,line) for index,line in enumerate(lines)]\n",
    "    return line_lengths\n",
    "\n",
    "@pandas_udf(ArrayType(StructType([\n",
    "    StructField(\"raw_line_id\", IntegerType(), False),\n",
    "    StructField(\"raw_line\", StringType(), False)\n",
    "])), PandasUDFType.SCALAR)\n",
    "def split_udf(df):\n",
    "    return df.apply(split_raw_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee1931d-3af8-483b-b882-1a0881bc831a",
   "metadata": {},
   "source": [
    "### 4.2 udf 处理添加新字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48607c2b-bf19-4f72-ac67-41138e05c42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "+--------------------+--------------------+--------------------+\n",
      "|                 url|         raw_content|       split_content|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|https://www.telel...|English\\tEnglish\\...|[{0, English\\tEng...|\n",
      "|http://www.ma.hu/...|hirdetés\\nma.hu n...|[{0, hirdetés}, {...|\n",
      "|http://angagement...|→ по-русски\\nCost...|[{0, → по-русски}...|\n",
      "|http://resistther...|Unwanted Resistan...|[{0, Unwanted Res...|\n",
      "|http://klimadiagr...|Das Klima in Karl...|[{0, Das Klima in...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:1.2108123302459717s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "split_result=spark_df.withColumn(\"split_content\", split_udf(spark_df['raw_content']))\n",
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    split_result.select(\"url\",\"raw_content\",\"split_content\").show(5)\n",
    "    e = time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce10792c-0dd1-4a8d-ac02-6890ea288c0a",
   "metadata": {},
   "source": [
    "### 4.3 将新字段展开获取paragraph级别row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d270efed-56f2-4214-9f30-0167a736defe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "DataFrame[summary: string, url: string, date_download: string, digest: string, length: string, nlines: string, source_domain: string, title: string, raw_content: string, raw_line_id: string, raw_line: string]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+------------------------+\n",
      "|                 url|         raw_content|raw_line_id|                raw_line|\n",
      "+--------------------+--------------------+-----------+------------------------+\n",
      "|https://www.telel...|English\\tEnglish\\...|          0|    English\\tEnglish\\ten|\n",
      "|https://www.telel...|English\\tEnglish\\...|          1|繁體中文\\tChinese (Tr...|\n",
      "|https://www.telel...|English\\tEnglish\\...|          2|                    Home|\n",
      "|https://www.telel...|English\\tEnglish\\...|          3|                Products|\n",
      "|https://www.telel...|English\\tEnglish\\...|          4|              Digital TV|\n",
      "+--------------------+--------------------+-----------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:1.4001696109771729s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Explode the split_content column and select the desired columns\n",
    "exploded_df = split_result.select(\"url\",\"date_download\",\"digest\",\"length\",\"nlines\",\"source_domain\",\"title\",\"raw_content\", explode(split_result.split_content).alias(\"exploded_content\"))\n",
    "\n",
    "# # Split the exploded_content struct into separate columns\n",
    "exploded_df = exploded_df.withColumn(\"raw_line_id\", exploded_df.exploded_content.raw_line_id)\n",
    "exploded_df = exploded_df.withColumn(\"raw_line\", exploded_df.exploded_content.raw_line)\n",
    "\n",
    "# Drop the exploded_content column if needed\n",
    "exploded_df = exploded_df.drop(\"exploded_content\")\n",
    "\n",
    "if(mode_para[\"isTest\"]):\n",
    "    exploded_df.cache()\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(exploded_df.summary())\n",
    "    exploded_df.select(\"url\",\"raw_content\",\"raw_line_id\",\"raw_line\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4f3de1-3b83-4e5e-a3ba-2cdf0eb212f4",
   "metadata": {},
   "source": [
    "### 4.4 添加hash 列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "004716b2-a56f-43eb-b150-3a42fab07775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASH_SIZE:8\n",
      "=== TestMode Log:\n",
      "DataFrame[summary: string, url: string, date_download: string, digest: string, length: string, nlines: string, source_domain: string, title: string, raw_content: string, raw_line_id: string, raw_line: string]\n",
      "+--------------------+--------------------+--------------------+------+------+----------------+--------------------+--------------------+-----------+------------------------+--------------------+\n",
      "|                 url|       date_download|              digest|length|nlines|   source_domain|               title|         raw_content|raw_line_id|                raw_line|          hash_value|\n",
      "+--------------------+--------------------+--------------------+------+------+----------------+--------------------+--------------------+-----------+------------------------+--------------------+\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4669|   111|www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|          0|    English\\tEnglish\\ten|[A7 E1 3C F2 70 F...|\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4669|   111|www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|          1|繁體中文\\tChinese (Tr...|[3E DB 9E EF B5 2...|\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4669|   111|www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|          2|                    Home|[E8 32 49 BD 3B A...|\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4669|   111|www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|          3|                Products|[FB DC 4F 23 F9 3...|\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4669|   111|www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|          4|              Digital TV|[59 44 27 AB 00 F...|\n",
      "+--------------------+--------------------+--------------------+------+------+----------------+--------------------+--------------------+-----------+------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:0.22984743118286133s\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BinaryType\n",
    "from ccnet_spark import normalize_for_dedup\n",
    "from typing import Iterable, Iterator, Sequence, Sized, Tuple, Type\n",
    "HASH_TYPE: Type[np.uint64] = np.uint64\n",
    "HASH_SIZE = HASH_TYPE(0).nbytes \n",
    "print(f\"HASH_SIZE:{HASH_SIZE}\") # 8 Byte ==> 64bit\n",
    "\n",
    "def compute_hashes(line):\n",
    "    if not line:\n",
    "        return None\n",
    "    normalized_line = normalize_for_dedup(line)  # Assuming normalize_for_dedup is defined\n",
    "    line_hash = hashlib.sha1(bytes(normalized_line, encoding=\"utf-8\")).digest()[:HASH_SIZE]\n",
    "    return line_hash\n",
    "\n",
    "@pandas_udf(BinaryType(),PandasUDFType.SCALAR)\n",
    "def udf_compute_hashes(line):\n",
    "    return line.apply(compute_hashes)\n",
    "# Assuming you have a dataframe named 'df' with a 'raw_line' column\n",
    "hash_df = exploded_df.withColumn(\"hash_value\", udf_compute_hashes(exploded_df.raw_line))\n",
    "\n",
    "# Show the resulting dataframe\n",
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(hash_df.summary())\n",
    "    hash_df.show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5de597-b7c1-435e-a40a-faa445e2df52",
   "metadata": {},
   "source": [
    "### 4.5根据 hash 去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2336b894-680f-42f2-b339-e101d37dfd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "DataFrame[summary: string, url: string, date_download: string, digest: string, length: string, nlines: string, source_domain: string, title: string, raw_content: string, raw_line_id: string, raw_line: string]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 17:29:19 WARN TaskSetManager: Stage 7 contains a task of very large size (1316 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+------------------------------+-----------+--------------------+\n",
      "|                 url|length|nlines|                   raw_content|raw_line_id|          hash_value|\n",
      "+--------------------+------+------+------------------------------+-----------+--------------------+\n",
      "|http://www.region...|  5765|   213|          Salta al contenut...|         22|[00 00 B7 6B E5 F...|\n",
      "|http://www.darulh...| 28049|   839|          Slå på/av meny\\nD...|        738|[00 02 2B BA 78 8...|\n",
      "|http://outdoormag...|  5695|   156|          O nas\\nRedakcja\\n...|        128|[00 02 FD 23 BA F...|\n",
      "|http://stk1031.bl...|  4265|   246|きまぐれあれやこれ\\n2018年5...|        167|[00 06 62 88 AC 5...|\n",
      "|https://jakandjil...|  3258|   305|          A moda está em tu...|         21|[00 08 83 89 3F 2...|\n",
      "+--------------------+------+------+------------------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:27.141693115234375s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "deduplicated_df = hash_df.dropDuplicates(['hash_value'])\n",
    "# Show the resulting dataframe\n",
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    deduplicated_df.cache()\n",
    "    s=time.time()\n",
    "    print(deduplicated_df.summary())\n",
    "    deduplicated_df.select(\"url\",\"length\",\"nlines\",\"raw_content\",\"raw_line_id\",\"hash_value\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130a3ec-292c-4877-8d68-7f242d076fe6",
   "metadata": {},
   "source": [
    "### 4.6 聚合\n",
    "将段落重新聚合为doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4823b-3f17-43a7-aed6-2fe06ae176b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\"url\",\"date_download\",\"digest\",\"length\",\"nlines\",\"source_domain\",\"title\",\"raw_content\",\n",
    "group_df = deduplicated_df.groupBy(\"digest\").agg(\n",
    "    F.first(\"url\").alias(\"url\"),\n",
    "    F.first(\"date_download\").alias(\"date_download\"),\n",
    "    F.first(\"source_domain\").alias(\"source_domain\"),\n",
    "    F.first(\"length\").alias(\"original_length\"),\n",
    "    F.first(\"nlines\").alias(\"original_nlines\"),\n",
    "    F.first(\"title\").alias(\"title\"),\n",
    "    F.concat_ws(\"\\n\", F.collect_list(\"raw_line\").alias(\"raw_content\")).alias(\"raw_content\"),\n",
    "    F.count(\"raw_line_id\").alias(\"nlines\"),\n",
    "    F.collect_list(\"raw_line_id\").alias(\"line_ids\"),\n",
    ")\n",
    "group_df=group_df.withColumn(\"length\", F.length(group_df[\"raw_content\"]))\n",
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    group_df.cache()\n",
    "    s=time.time()\n",
    "    group_df.select(\"url\",\"original_length\",\"original_nlines\",\"raw_content\",\"length\",\"nlines\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2caa63b-6d25-4237-8c7d-4b7418e0240b",
   "metadata": {},
   "source": [
    "### 4.7 计算留存比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffc671f-05c3-4be5-adce-7da277d03cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode_para[\"isTest\"]:\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    origin_chars = spark_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    remain_chars = group_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    e = time.time()\n",
    "    print(f\"origin chars:{origin_chars/1000/1000}M,remain_chars:{remain_chars/1000/1000}M \\n \\\n",
    "            keep chars:{round(remain_chars/origin_chars*100,3)} % time consume:{e-s}\")\n",
    "else:\n",
    "    print(\"=== DevMode Log:\")\n",
    "    s = time.time()\n",
    "    origin_chars = spark_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    remain_chars = group_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    e = time.time()\n",
    "    print(f\"origin chars:{origin_chars/1000/1000}M,remain_chars:{remain_chars/1000/1000}M \\n \\\n",
    "            keep chars:{round(remain_chars/origin_chars*100,3)} % time consume:{e-s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056b4824-3eb8-47f5-8d84-d479406688db",
   "metadata": {},
   "source": [
    "## 5. 语言识别导入\n",
    "\n",
    "Optimized Solution\n",
    "The solution is then to cache the object initialization. For this, we need the cachetools library. On Databricks, you can install it by running the following cell\n",
    "\n",
    "%`pip install cachetools`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef8cc11-9a51-4e70-8005-1602362d0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, FloatType\n",
    "\n",
    "import fasttext  # type: ignore\n",
    "from cachetools import cached\n",
    "\n",
    "@cached(cache={})\n",
    "def getFastTextModel():\n",
    "    model_path = \"models/fasttext/lid.bin\"\n",
    "    fasttext_model = fasttext.load_model(model_path)\n",
    "    return fasttext_model\n",
    "def predict(model, text: str, k: int = 1):\n",
    "    labels, scores = model.predict(text, k=k)\n",
    "    labels = [l.replace(\"__label__\", \"\") for l in labels]\n",
    "    return labels, scores\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def predictLang(text):\n",
    "    if not text:\n",
    "        return None\n",
    "    labels, scores = predict(getFastTextModel(), text.replace(\"\\n\", \"\"), k=1)\n",
    "    scores.round(2, out=scores)\n",
    "    lang = labels[0]\n",
    "    score = scores[0]\n",
    "    if score < 0.5:\n",
    "        return None\n",
    "    return lang\n",
    "\n",
    "lang_df = group_df.withColumn(\"lang\", predictLang(\"raw_content\"))\n",
    "if mode_para[\"isTest\"]:\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    lang_df.select(\"url\",\"raw_content\",\"lang\").show(20)\n",
    "    e = time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a66705-738a-4999-a6d2-ed20351e20d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6dc1fd-b16b-4dbc-b929-9c87bd7eea20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
