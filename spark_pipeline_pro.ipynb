{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45bf866-59e7-4fa1-b3f3-0eac31f300f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:====>                                                   (1 + 11) / 12]\r"
     ]
    }
   ],
   "source": [
    "from ccnet_spark.text_normalizer import normalize\n",
    "from ccnet_spark.pipe_preprocess import load_segments\n",
    "from ccnet_spark.pipe_hash import compute_hashes,split_doc2para\n",
    "from ccnet_spark.pipe_lid import predictLang,predictScore\n",
    "from ccnet_spark.pipe_tokenized import doSentencePiece\n",
    "from ccnet_spark.pipe_perplexity import doDocLM\n",
    "from ccnet_spark.pipe_ppbucket import doPPBucket\n",
    "from ccnet_spark.pipe_save import save_partation,load_partation\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"CCNETSpark\")  \\\n",
    "                    .config(\"spark.executor.memory\", \"100g\") \\\n",
    "                    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "                    .config(\"spark.driver.maxResultSize\", \"32g\") \\\n",
    "                    .config('spark.sql.execution.arrow.pyspark.enabled', 'true') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6033320f-74a2-41e5-91b8-e1c0bd6ea14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModePara(mode):\n",
    "    if(mode==\"test\"):\n",
    "        cache_folder=\"/root/wxl_folder/cache_data/\"\n",
    "        date=\"2019-09\" ## hardcode ,现在只能是这个\n",
    "        segments=[i for i in range(10)]\n",
    "        min_len=300\n",
    "        isSample=True\n",
    "        sampleRate=0.01\n",
    "    else:\n",
    "        cache_folder=\"/root/wxl_folder/cache_data/\"\n",
    "        date=\"2019-09\" ## hardcode ,现在只能是这个\n",
    "        segments=[i for i in range(10)]\n",
    "        min_len=300\n",
    "        isSample=True\n",
    "        sampleRate=0.01\n",
    "    return [cache_folder,date,segments,min_len,isSample,sampleRate]\n",
    "mode=\"test\"\n",
    "cache_folder,date,segments,min_len,isSample,sampleRate=getModePara(mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cb8b4a6-d77d-4e06-bb9a-ffc38f461fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 14:36 INFO 716147:root - load segment 0, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 14:36 INFO 716147:root - load segment 1, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 14:36 INFO 716147:root - load segment 2, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 14:36 INFO 716147:root - load segment 3, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 14:36 INFO 716147:root - load segment 4, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 14:36 INFO 716147:root - load segment 5, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 14:36 INFO 716147:root - load segment 6, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 14:36 INFO 716147:root - load segment 7, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 14:36 INFO 716147:root - load segment 8, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 14:36 INFO 716147:root - load segment 9, with sampleRate:1.0%,min_len:300,with date:2019-09\n"
     ]
    }
   ],
   "source": [
    "spark_df=load_segments(spark,segments,cache_folder,date=date,isSample=isSample,sampleRate=sampleRate,min_len=min_len)\n",
    "spark_df=spark_df.withColumn(\"length\", F.length(spark_df[\"raw_content\"]))\n",
    "split_result = spark_df.withColumn(\"split_content\", split_doc2para(spark_df[\"raw_content\"]))\n",
    "exploded_df=split_result.withColumn(\"exploded_content\", explode(split_result.split_content))\n",
    "exploded_df = exploded_df.withColumn(\"raw_line_id\", exploded_df.exploded_content.raw_line_id) \\\n",
    "                         .withColumn(\"raw_line\", exploded_df.exploded_content.raw_line) \\\n",
    "                         .drop(\"exploded_content\")\n",
    "hash_df = exploded_df.withColumn(\"hash_value\", compute_hashes(exploded_df.raw_line))\n",
    "deduplicated_df = hash_df.dropDuplicates(['hash_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c76d1e5-48e4-44bd-b015-c7f66d39277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df = deduplicated_df.groupBy(\"digest\").agg(\n",
    "    F.first(\"url\").alias(\"url\"),\n",
    "    F.first(\"date_download\").alias(\"date_download\"),\n",
    "    F.first(\"source_domain\").alias(\"source_domain\"),\n",
    "    F.first(\"cc_segment\").alias(\"cc_segment\"),\n",
    "    F.first(\"length\").alias(\"original_length\"),\n",
    "    F.first(\"nlines\").alias(\"original_nlines\"),\n",
    "    F.first(\"title\").alias(\"title\"),\n",
    "    F.concat_ws(\"\\n\", F.collect_list(\"raw_line\").alias(\"raw_content\")).alias(\"raw_content\"),\n",
    "    F.count(\"raw_line_id\").alias(\"nlines\"),\n",
    "    F.collect_list(\"raw_line_id\").alias(\"line_ids\"),\n",
    ")\n",
    "group_df=group_df.withColumn(\"length\", F.length(group_df[\"raw_content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "762eaae8-2808-428a-9ee5-ea0ff9fe255b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_df = group_df.withColumn(\"lang\", predictLang(\"raw_content\"))\n",
    "lang_df = lang_df.withColumn(\"score\", predictScore(\"raw_content\"))\n",
    "lm_df = lang_df.withColumn(\"tokenized\", doSentencePiece(\"raw_content\",\"lang\"))\n",
    "doclm_df = lm_df.withColumn(\"perplexity\", doDocLM(\"tokenized\",\"lang\"))\n",
    "bucket_df = doclm_df.withColumn(\"bucket\", doPPBucket(\"perplexity\",\"lang\"))\n",
    "drop_df = bucket_df.drop(\"tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4be083ca-6921-4b4f-b410-c3202be814e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "2024-04-02 14:39 ERROR 716147:root - KeyboardInterrupt while sending command.12]\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/ccnet_spark/lib/python3.9/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/root/miniconda3/envs/ccnet_spark/lib/python3.9/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/root/miniconda3/envs/ccnet_spark/lib/python3.9/socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n",
      "2024-04-02 14:39 INFO 716147:py4j.clientserver - Closing down clientserver connection\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msave_partation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43misSample\u001b[49m\u001b[43m,\u001b[49m\u001b[43msampleRate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmin_len\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/wxl_folder/ccnet_spark/ccnet_spark/pipe_save.py:25\u001b[0m, in \u001b[0;36msave_partation\u001b[0;34m(spark_df, cache_folder, date, isSample, sampleRate, min_len)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_path):\n\u001b[1;32m     24\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(output_path\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mspark_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitionBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlang\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbucket\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfile:///\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43moutput_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ccnet_spark/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[0;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ccnet_spark/lib/python3.9/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/miniconda3/envs/ccnet_spark/lib/python3.9/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/miniconda3/envs/ccnet_spark/lib/python3.9/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ccnet_spark/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "save_partation(drop_df,cache_folder,date,isSample,sampleRate,min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8117ccf3-6fd0-44bd-a183-4b7fc975ff56",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_bucket=\"head\"\n",
    "selected_lang=\"en\"\n",
    "df_en_head=load_partation(spark,selected_lang,selected_bucket,cache_folder,date,isSample,sampleRate,min_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc078fed-6833-485d-87a1-1a7219288e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en_head.select(\"url\",\"raw_content\",\"perplexity\",\"length\",\"cc_segment\").show()\n",
    "print(df_en_head.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a913b-c55a-4112-a5e0-f1a0c87cbcd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfab746d-ee94-4a3d-a3f1-4df5103f589e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaea937-a756-49c5-9579-e13097b5a771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
