{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2594801d-1520-420e-bb79-26414ae769a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/07 15:32:46 WARN Utils: Your hostname, MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.110.96 instead (on interface en0)\n",
      "24/04/07 15:32:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/07 15:32:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from ccnet_spark.pipeline import Pipeline,Config\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7a49aee-887e-446a-b6e4-f0404180e263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(dump='2019-18', cache_dir='../cache_data/', output_dir='../cache_data/', min_len=300, isSample=True, sampleRate=0.001, n_segments=1, pipeline=['real_len', 'hash', 'dedup_keep', 'lid', 'sp', 'lm', 'pp_bucket', 'drop'], spark=<pyspark.sql.session.SparkSession object at 0x11e48f100>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config=Config(isSample=True,n_segments=1,sampleRate=0.001,cache_dir=\"../cache_data/\",dump=\"2019-18\")\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9d0c037-21bb-432f-a1a7-cb4cb00f41e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/07 15:32:53 WARN DataSource: All paths were ignored:\n",
      "  file:/Users/zz/github/cache_data/sdf_parquet/2019-18/_sampleRate_0_segment_0_min_len_300.parquet\n",
      "2024-04-07 15:32 INFO 5912:root - load segment 0, with sampleRate:0.1%,min_len:300,with date:2019-18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8398339748382568\n"
     ]
    }
   ],
   "source": [
    "pipeline=Pipeline(config)\n",
    "df=pipeline.load_data()\n",
    "s=time.time()\n",
    "pipeline.run()\n",
    "pipeline.save_data()\n",
    "e=time.time()\n",
    "print(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b29aeb9a-73d5-4ad5-a4ca-53a9005a6159",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, digest: string, url: string, date_download: string, source_domain: string, cc_segment: string, original_length: string, original_nlines: string, title: string, nlines: string, raw_content: string, length: string, lang: string, score: string, perplexity: string, bucket: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "789238dd-e865-4327-bec6-dee25dcc399c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n",
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-----+-------------------+----------+----------+-------------------+\n",
      "|bucket|lang|count|sum_original_length|sum_length|sum_nlines|sum_original_nlines|\n",
      "+------+----+-----+-------------------+----------+----------+-------------------+\n",
      "|middle|  en|    2|               8413|      8222|       113|                130|\n",
      "|  tail|  nl|    1|              17259|     16452|       224|                285|\n",
      "|  tail|  hu|    1|               4798|      4152|        87|                109|\n",
      "|  tail|  es|    2|              14823|     13461|       214|                302|\n",
      "|  tail|  de|    2|               7849|      5284|       143|                230|\n",
      "|  tail|  ru|    3|              19653|     17655|       286|                372|\n",
      "|  tail|  it|    1|               6663|      6000|        60|                 78|\n",
      "|  tail|  en|   12|              54707|     41535|      1142|               2020|\n",
      "|  tail|  ro|    1|               1280|      1128|        44|                 52|\n",
      "|  tail|  ja|    2|               4662|      3647|       184|                262|\n",
      "|   all|NULL|    6|              60167|     37368|      1052|               2430|\n",
      "|  tail|  ar|    1|               2206|      2194|        62|                 65|\n",
      "|  tail|  zh|    4|              46158|     45158|       313|                405|\n",
      "|middle|  de|    1|               1130|      1130|        18|                 18|\n",
      "|  tail|  pl|    1|               7803|      5735|       264|                411|\n",
      "|  head|  en|    2|              10827|     10420|        90|                117|\n",
      "+------+----+-----+-------------------+----------+----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 定义聚合函数求和\n",
    "sum_columns = [\n",
    "    F.sum('original_length').alias('sum_original_length'),\n",
    "    F.sum('length').alias('sum_length'),\n",
    "    F.sum('nlines').alias('sum_nlines'),\n",
    "    F.sum('original_nlines').alias('sum_original_nlines')\n",
    "]\n",
    "\n",
    "# 按照 'bucket' 和 'lang' 字段进行分组，并计算每个组合的数量和求和\n",
    "aggregated_df = pipeline.df.groupBy('bucket', 'lang').agg(\n",
    "    F.count('*').alias('count'), *sum_columns\n",
    ")\n",
    "\n",
    "# 显示聚合后的结果\n",
    "aggregated_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24112ade-f36c-42c7-96dd-da6c75d98c35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
