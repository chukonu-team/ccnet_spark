{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86aaf72e-5fba-43eb-b10c-31aa56d42206",
   "metadata": {},
   "source": [
    "# ccnet spark pipeline 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e7aa31-b893-45a3-aa85-450f056caf1f",
   "metadata": {},
   "source": [
    "## 1. 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad5b292-4eb3-4cc0-8837-2a8cf03fd7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/02 15:47:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/04/02 15:47:05 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from ccnet_spark.text_normalizer import normalize\n",
    "from ccnet_spark.pipe_preprocess import load_segments\n",
    "from ccnet_spark.pipe_hash import compute_hashes,split_doc2para\n",
    "from ccnet_spark.pipe_lid import predictLang\n",
    "from ccnet_spark.pipe_tokenized import doSentencePiece\n",
    "from ccnet_spark.pipe_perplexity import doDocLM\n",
    "from ccnet_spark.pipe_ppbucket import doPPBucket\n",
    "from ccnet_spark.pipe_save import save_partation,load_partation\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"CCNETSpark\")  \\\n",
    "                    .config(\"spark.executor.memory\", \"64g\") \\\n",
    "                    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "                    .config(\"spark.driver.maxResultSize\", \"32g\") \\\n",
    "                    .config('spark.sql.execution.arrow.pyspark.enabled', 'true') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fa1fc3-e154-4503-9632-8054923913d6",
   "metadata": {},
   "source": [
    "## 2. 参数配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51124368-58d7-417d-bc7e-9f7f164c4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModePara(mode):\n",
    "    if(mode==\"test\"):\n",
    "        n_segments=10\n",
    "        cache_folder=\"/root/wxl_folder/cache_data/\"\n",
    "        date=\"2019-09\" ## hardcode ,现在只能是这个\n",
    "        segments=[i for i in range(n_segments)]\n",
    "        min_len=300\n",
    "        isSample=True\n",
    "        sampleRate=0.01\n",
    "        num_partitions=1\n",
    "    else:\n",
    "        n_segments=4\n",
    "        cache_folder=\"/root/wxl_folder/cache_data/\"\n",
    "        date=\"2019-09\" ## hardcode ,现在只能是这个\n",
    "        segments=[i for i in range(n_segments)]\n",
    "        min_len=300\n",
    "        isSample=False\n",
    "        sampleRate=1\n",
    "        num_partitions=4\n",
    "    return [cache_folder,date,segments,min_len,isSample,sampleRate,num_partitions]\n",
    "mode=\"test\"\n",
    "cache_folder,date,segments,min_len,isSample,sampleRate,num_partitions=getModePara(mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99a057-a8db-4d94-b772-988434ac58cc",
   "metadata": {},
   "source": [
    "## 2.1 读取文件数据，处理成spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5466b05b-1f87-4e6e-ba97-da100fe691ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 15:47 INFO 759634:root - load segment 0, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 15:47 INFO 759634:root - load segment 1, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 15:47 INFO 759634:root - load segment 2, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 15:47 INFO 759634:root - load segment 3, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 15:47 INFO 759634:root - load segment 4, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 15:47 INFO 759634:root - load segment 5, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 15:47 INFO 759634:root - load segment 6, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 15:47 INFO 759634:root - load segment 7, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 15:47 INFO 759634:root - load segment 8, with sampleRate:1.0%,min_len:300,with date:2019-09\n",
      "2024-04-02 15:47 INFO 759634:root - load segment 9, with sampleRate:1.0%,min_len:300,with date:2019-09\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 10 segments,time consume:3.3463284969329834s\n"
     ]
    }
   ],
   "source": [
    "s=time.time()\n",
    "spark_df=load_segments(spark,segments,cache_folder,date=date,isSample=isSample,sampleRate=sampleRate,min_len=min_len)\n",
    "# spark_df = spark_df.repartition(num_partitions, \"cc_segment\")  # 使用哈希分区，\"column_name\" 是分区键\n",
    "e=time.time()\n",
    "print(f\"load {len(segments)} segments,time consume:{e-s}s\")\n",
    "if(mode==\"test\"):\n",
    "    doc_count=spark_df.count()\n",
    "    print(f\"load {doc_count} docs from:{len(segments)} segments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e1f0c-f711-4b54-a2f4-7317326414ed",
   "metadata": {},
   "source": [
    "## 3 字段分析\n",
    "1. wet 文件本身带有长度：\"length\": length,这个是从wet的\"Content-Length:\"读出来的，和我计算len(raw_content）有出入。考虑原因是原先的length不只是说raw_content，还包括title等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10dbbad-2058-4308-91c4-74edfee456e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(mode==\"test\"):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(spark_df.summary())\n",
    "    tmp_df = spark_df.withColumn(\"compute_length\", F.length(spark_df[\"raw_content\"]))\n",
    "    tmp_df.select(\"url\",\"length\",\"raw_content\",\"title\",\"nlines\",\"compute_length\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d8ff6e-69db-4412-92c5-a4783623c124",
   "metadata": {},
   "source": [
    "### 3.1 修改length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05383d-fc9d-4af6-b8be-7b74ce80ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df=spark_df.withColumn(\"length\", F.length(spark_df[\"raw_content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9793de8-008f-4aec-9dc7-0bbad27feff3",
   "metadata": {},
   "source": [
    "## 4. hash计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac16adaa-b1c1-4e99-8bdf-93312621e5f4",
   "metadata": {},
   "source": [
    "### 4.2 udf 处理添加新字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c505bf4-b9f4-4e1f-ad51-ad56939b28a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假设spark_df是您的DataFrame\n",
    "# 使用UDF对raw_content字段进行处理\n",
    "split_result = spark_df.withColumn(\"split_content\", split_doc2para(spark_df[\"raw_content\"]))\n",
    "if(mode==\"test\"):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(split_result.summary())\n",
    "    split_result.select(\"url\",\"length\",\"nlines\",\"raw_content\",\"split_content\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd212cd6-25ca-434a-b633-01ca1e7b7ecd",
   "metadata": {},
   "source": [
    "### 4.3 将新字段展开获取paragraph级别row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f305dc-6e04-40a9-8138-d3049dc4c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded_df=split_result.withColumn(\"exploded_content\", explode(split_result.split_content))\n",
    "exploded_df = exploded_df.withColumn(\"raw_line_id\", exploded_df.exploded_content.raw_line_id) \\\n",
    "                         .withColumn(\"raw_line\", exploded_df.exploded_content.raw_line) \\\n",
    "                         .drop(\"exploded_content\")\n",
    "if(mode==\"test\"):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(exploded_df.summary())\n",
    "    exploded_df.select(\"url\",\"raw_content\",\"raw_line_id\",\"raw_line\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a82cec-45d4-4121-b570-c38091c425fd",
   "metadata": {},
   "source": [
    "### 4.4 添加hash 列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d74e40-0f12-4556-bac2-0bd91c94cada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a dataframe named 'df' with a 'raw_line' column\n",
    "hash_df = exploded_df.withColumn(\"hash_value\", compute_hashes(exploded_df.raw_line))\n",
    "\n",
    "# Show the resulting dataframe\n",
    "if(mode==\"test\"):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(hash_df.summary())\n",
    "    hash_df.show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0d5a4-2cd6-4925-8f53-690b0213bf9c",
   "metadata": {},
   "source": [
    "### 4.5根据 hash 去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620aaf6b-9774-4a6f-98a2-962db1f9a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "deduplicated_df = hash_df.dropDuplicates(['hash_value'])\n",
    "# Show the resulting dataframe\n",
    "if(mode==\"test\"):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(deduplicated_df.summary())\n",
    "    deduplicated_df.select(\"url\",\"length\",\"nlines\",\"raw_content\",\"raw_line_id\",\"hash_value\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ed5d3-f4b3-4d94-a663-6c9279071b5d",
   "metadata": {},
   "source": [
    "### 4.6 聚合\n",
    "将段落重新聚合为doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e9c8fc-fdd2-4d71-81ae-77b6ebf77805",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_df = deduplicated_df.groupBy(\"digest\").agg(\n",
    "    F.first(\"url\").alias(\"url\"),\n",
    "    F.first(\"date_download\").alias(\"date_download\"),\n",
    "    F.first(\"source_domain\").alias(\"source_domain\"),\n",
    "    F.first(\"cc_segment\").alias(\"cc_segment\"),\n",
    "    F.first(\"length\").alias(\"original_length\"),\n",
    "    F.first(\"nlines\").alias(\"original_nlines\"),\n",
    "    F.first(\"title\").alias(\"title\"),\n",
    "    F.concat_ws(\"\\n\", F.collect_list(\"raw_line\").alias(\"raw_content\")).alias(\"raw_content\"),\n",
    "    F.count(\"raw_line_id\").alias(\"nlines\"),\n",
    "    F.collect_list(\"raw_line_id\").alias(\"line_ids\"),\n",
    ")\n",
    "group_df=group_df.withColumn(\"length\", F.length(group_df[\"raw_content\"]))\n",
    "if(mode==\"test\"):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    group_df.cache()\n",
    "    s=time.time()\n",
    "    group_df.select(\"url\",\"original_length\",\"original_nlines\",\"raw_content\",\"length\",\"nlines\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59366d-6d0e-4ab8-937b-1dbcb39a1304",
   "metadata": {},
   "source": [
    "### 4.7 计算留存比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1295a70-8dce-40c1-aad2-103efb493626",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(mode==\"test\"):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    origin_chars = spark_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    remain_chars = group_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    e = time.time()\n",
    "    print(f\"origin chars:{origin_chars/1000/1000}M,remain_chars:{remain_chars/1000/1000}M \\n \\\n",
    "            keep chars:{round(remain_chars/origin_chars*100,3)} % time consume:{e-s}\")\n",
    "else:\n",
    "    print(\"=== DevMode Log:\")\n",
    "    s = time.time()\n",
    "    origin_chars = spark_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    remain_chars = group_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    e = time.time()\n",
    "    print(f\"origin chars:{origin_chars/1000/1000}M,remain_chars:{remain_chars/1000/1000}M \\n \\\n",
    "            keep chars:{round(remain_chars/origin_chars*100,3)} % time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1063646-dc67-44d2-acfd-80be5ba32b88",
   "metadata": {},
   "source": [
    "## 5. 语言识别导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4f573-245e-483b-a9d8-d8c79f2097fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_df = group_df.withColumn(\"lang_score\", predictLang(\"raw_content\"))\n",
    "lang_df = lang_df.withColumn(\"lang\", lang_df.lang_score.lang) \\\n",
    "                         .withColumn(\"score\", lang_df.lang_score.score) \\\n",
    "                         .drop(\"lang_score\")\n",
    "if(mode==\"test\"):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    lang_df.select(\"url\",\"raw_content\",\"lang\",\"score\").show(5)\n",
    "    e = time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875128c6-b78c-4ffb-b9b6-aa5fc13e0220",
   "metadata": {},
   "source": [
    "## 6. MultiSentencePiece 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd953788-afe5-4b0a-9240-b703612eefe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_df = lang_df.withColumn(\"tokenized\", doSentencePiece(\"raw_content\",\"lang\"))\n",
    "if(mode==\"test\"):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    lm_df.select(\"url\",\"raw_content\",\"lang\",\"score\",\"tokenized\").show(5)\n",
    "    e = time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad98f5-b0ab-4dea-a631-d12920e92e6b",
   "metadata": {},
   "source": [
    "## 7. 困惑度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73420a13-2620-48e5-8ee4-f2db7747d75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doclm_df = lm_df.withColumn(\"perplexity\", doDocLM(\"tokenized\",\"lang\"))\n",
    "if(mode==\"test\"):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    doclm_df.select(\"url\",\"raw_content\",\"lang\",\"score\",\"tokenized\",\"perplexity\").show(5)\n",
    "    e = time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c629b727-1d96-4f15-95cd-897a5ff37df7",
   "metadata": {},
   "source": [
    "## 8. PerplexityBucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3740bb02-7c9c-4931-9d94-8a4ff2da3eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_df = doclm_df.withColumn(\"bucket\", doPPBucket(\"perplexity\",\"lang\"))\n",
    "if(mode==\"test\"):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    bucket_df.select(\"url\",\"raw_content\",\"lang\",\"score\",\"tokenized\",\"perplexity\",\"bucket\").show(50)\n",
    "    e = time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dc2c90-c41d-4c42-a893-0131f7537b3c",
   "metadata": {},
   "source": [
    "## 9. dropKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a1a31e-7e6f-460e-95bd-31580e7bc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_df = bucket_df.drop(\"tokenized\")\n",
    "if(mode==\"test\"):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    print(drop_df.summary())\n",
    "    e = time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12648326-55ec-4cb4-80d3-abffa04bb16b",
   "metadata": {},
   "source": [
    "## 10. split by lang:save & load partation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e58f31-8afd-40e8-8803-540a4e391035",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_partation(drop_df,cache_folder,date,isSample,sampleRate,min_len)\n",
    "selected_bucket=\"head\"\n",
    "selected_lang=\"en\"\n",
    "df_en_head=load_partation(spark,selected_lang,selected_bucket,cache_folder,date,isSample,sampleRate,min_len)\n",
    "df_en_head.select(\"url\",\"raw_content\",\"perplexity\",\"length\",\"cc_segment\").show()\n",
    "print(df_en_head.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc24ff38-fb2b-4c30-9f83-7c43a5e35b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a761e1-30fd-4c17-a93e-c32b5d3a2945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
