{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86aaf72e-5fba-43eb-b10c-31aa56d42206",
   "metadata": {},
   "source": [
    "# ccnet spark pipeline 实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e7aa31-b893-45a3-aa85-450f056caf1f",
   "metadata": {},
   "source": [
    "## 1. 导入依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ad5b292-4eb3-4cc0-8837-2a8cf03fd7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccnet_spark import open_read, parse_warc_file,compute_hashes,NaiveHashSet\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import ArrayType, StringType,IntegerType,StructType, StructField\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import udf, explode\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "# 初始化 SparkSession\n",
    "spark = SparkSession.builder.appName(\"CCNETSpark\")  \\\n",
    "                    .config(\"spark.executor.memory\", \"110g\") \\\n",
    "                    .config(\"spark.driver.memory\", \"32g\") \\\n",
    "                    .config(\"spark.driver.maxResultSize\", \"10g\") \\\n",
    "                    .getOrCreate()\n",
    "# spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f99a057-a8db-4d94-b772-988434ac58cc",
   "metadata": {},
   "source": [
    "## 2. 读取文件数据，处理成pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493e968-ee52-488d-bab9-5c54cff63dd6",
   "metadata": {},
   "source": [
    "### 2.1 获取cache文件路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb17ca6-b75b-4307-8083-d5e48e5cd768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../cache_data/2019-09/CC-MAIN-20190215183319-20190215205319-00003.warc.wet.gz\n"
     ]
    }
   ],
   "source": [
    "cache_data=\"../cache_data/2019-09/\"\n",
    "def getWETURL(segment: int):\n",
    "    cache_file_prefix = \"CC-MAIN-20190215183319-20190215205319-\"\n",
    "    cache_file_sufix = \".warc.wet.gz\"\n",
    "    segment_str = str(segment).zfill(5)  # Pad with leading zeros\n",
    "    return cache_data+cache_file_prefix + segment_str + cache_file_sufix\n",
    "url = getWETURL(3)\n",
    "print(url)  # Output: CC-MAIN-20190215183319-20190215205319-00003.warc.wet.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0530fda-e712-49b4-bab1-d9c76972b465",
   "metadata": {},
   "source": [
    "### 2.2 处理文件，存入pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ed3705e-128e-4f62-b4b8-f09dce29e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getpdf(segment,isPart:bool):\n",
    "    file_path=Path(getWETURL(segment))\n",
    "    file=open_read(file_path)\n",
    "    s=time.time()\n",
    "    pandas_df = parse_warc_file(file, 30)\n",
    "    if(isPart):\n",
    "        random_save_n=100\n",
    "        pandas_df = pandas_df.sample(n=random_save_n, random_state=1)\n",
    "    e=time.time()\n",
    "    print(f\"====== parse segment:{segment} to pd_df consume:{e-s} s\")\n",
    "    return pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e0145-8660-4e4d-ac8a-aab2fc41a16c",
   "metadata": {},
   "source": [
    "## 3. 读取 spark dataframe 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cefa63a6-47df-447c-8c36-f780233a16b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getsdf(segment,isPart:bool):\n",
    "    inner_path = \"_part\" if isPart else \"_all\"\n",
    "    output_path = cache_data+\"cache_parquet/\"+str(segment)+  inner_path +\".parquet\"  # 设置输出路径\n",
    "    # 检查本地文件是否存在\n",
    "    if not os.path.exists(output_path):\n",
    "        print(f\"======process to parquet of segment {segment}{inner_path}\")\n",
    "        # 处理文件并生成 Spark DataFrame\n",
    "        pdf = getpdf(segment,isPart=isPart)\n",
    "        pdf.to_parquet(output_path)  # 保存为 parquet 文件\n",
    "        spark_df = spark.createDataFrame(pdf)\n",
    "    else:\n",
    "        print(f\"======read parquet of segment {segment}{inner_path} from cache\")\n",
    "        pdf = pd.read_parquet(output_path)\n",
    "        spark_df = spark.createDataFrame(pdf)\n",
    "    return spark_df\n",
    "def getsdfs(segments,isPart:bool = False):\n",
    "    merged_sdf=None\n",
    "    for seg in segments:\n",
    "        if(merged_sdf):\n",
    "            merged_sdf = merged_sdf.unionAll(getsdf(seg,isPart)) # Merge DataFrames\n",
    "        else:\n",
    "            merged_sdf = getsdf(seg,isPart)\n",
    "    return merged_sdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef42772d-a0cd-44e7-b197-312deb3ef263",
   "metadata": {},
   "source": [
    "### 3.1 load spark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baf88a49-9535-4646-ac40-b2d23d16c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModePara(mode):\n",
    "    if(mode==\"test\"):\n",
    "        para={\n",
    "            \"isTest\":True,\n",
    "            \"isPart\":True,\n",
    "            \"segments\":5,\n",
    "        }\n",
    "        return para\n",
    "    else:\n",
    "        para={\n",
    "            \"isTest\":False,\n",
    "            \"isPart\":False,\n",
    "            \"segments\":40,\n",
    "        }\n",
    "        return para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac2930d2-fa9b-420f-bc2d-5fc18322fc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode=\"test\"\n",
    "mode_para=getModePara(mode)\n",
    "segments=[i for i in range(mode_para[\"segments\"])]\n",
    "isPart=mode_para[\"isPart\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12719579-006a-4e30-bdad-a11cd62340b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======read parquet of segment 0_part from cache\n",
      "======read parquet of segment 1_part from cache\n",
      "======read parquet of segment 2_part from cache\n",
      "======read parquet of segment 3_part from cache\n",
      "======read parquet of segment 4_part from cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 10:46:04 WARN TaskSetManager: Stage 0 contains a task of very large size (1321 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 0:============================================>           (64 + 16) / 80]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load 5 segments,with 5000 docs,comsume:16.137235164642334s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "s=time.time()\n",
    "spark_df = getsdfs(segments,isPart=isPart)\n",
    "num_docs=spark_df.count()\n",
    "e=time.time()\n",
    "print(f\"load {len(segments)} segments,with {num_docs} docs,comsume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7e1f0c-f711-4b54-a2f4-7317326414ed",
   "metadata": {},
   "source": [
    "### 3.2 字段分析\n",
    "1. wet 文件本身带有长度：\"length\": length,这个是从wet的\"Content-Length:\"读出来的，和我计算len(raw_content）有出入。考虑是原先的length,不只是raw_content，还包括title等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c10dbbad-2058-4308-91c4-74edfee456e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "DataFrame[summary: string, url: string, date_download: string, digest: string, length: string, nlines: string, source_domain: string, title: string, raw_content: string]\n",
      "+--------------------+------+------+--------------+\n",
      "|                 url|length|nlines|compute_length|\n",
      "+--------------------+------+------+--------------+\n",
      "|https://www.telel...|  4758|   111|          4669|\n",
      "|http://www.ma.hu/...|  4180|    70|          3716|\n",
      "|http://angagement...|  1326|    65|          1231|\n",
      "|http://resistther...|   912|    23|           868|\n",
      "|http://klimadiagr...|  1918|    51|          1851|\n",
      "+--------------------+------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:1.8806560039520264s\n"
     ]
    }
   ],
   "source": [
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(spark_df.summary())\n",
    "    tmp_df = spark_df.withColumn(\"compute_length\", F.length(spark_df[\"raw_content\"]))\n",
    "    tmp_df.select(\"url\",\"length\",\"nlines\",\"compute_length\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d8ff6e-69db-4412-92c5-a4783623c124",
   "metadata": {},
   "source": [
    "### 3.3 修改length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b05383d-fc9d-4af6-b8be-7b74ce80ebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df=spark_df.withColumn(\"length\", F.length(spark_df[\"raw_content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9793de8-008f-4aec-9dc7-0bbad27feff3",
   "metadata": {},
   "source": [
    "## 4. hash计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dea9e5-b3a8-4b50-b40c-147fd7bbecbd",
   "metadata": {},
   "source": [
    "### 4.1 定义UDF,将doc 分割成paragraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44be1b10-248f-470d-874b-3b3f4962a3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个函数，用于分割文本\n",
    "def split_raw_content(content):\n",
    "    lines = content.split('\\n')\n",
    "    line_ids = range(0, len(lines))  # 生成行号\n",
    "    return list(zip(line_ids, lines))\n",
    "\n",
    "# 注册为UDF\n",
    "split_udf = udf(split_raw_content, ArrayType(StructType([\n",
    "    StructField(\"raw_line_id\", IntegerType(), False),\n",
    "    StructField(\"raw_line\", StringType(), False)\n",
    "])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac16adaa-b1c1-4e99-8bdf-93312621e5f4",
   "metadata": {},
   "source": [
    "### 4.2 udf 处理添加新字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c505bf4-b9f4-4e1f-ad51-ad56939b28a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "DataFrame[summary: string, url: string, date_download: string, digest: string, length: string, nlines: string, source_domain: string, title: string, raw_content: string]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+--------------------+--------------------+\n",
      "|                 url|length|nlines|         raw_content|       split_content|\n",
      "+--------------------+------+------+--------------------+--------------------+\n",
      "|https://www.telel...|  4669|   111|English\\tEnglish\\...|[{0, English\\tEng...|\n",
      "|http://www.ma.hu/...|  3716|    70|hirdetés\\nma.hu n...|[{0, hirdetés}, {...|\n",
      "|http://angagement...|  1231|    65|→ по-русски\\nCost...|[{0, → по-русски}...|\n",
      "|http://resistther...|   868|    23|Unwanted Resistan...|[{0, Unwanted Res...|\n",
      "|http://klimadiagr...|  1851|    51|Das Klima in Karl...|[{0, Das Klima in...|\n",
      "+--------------------+------+------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:3.4217610359191895s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 假设spark_df是您的DataFrame\n",
    "# 使用UDF对raw_content字段进行处理\n",
    "split_result = spark_df.withColumn(\"split_content\", split_udf(spark_df[\"raw_content\"]))\n",
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(split_result.summary())\n",
    "    split_result.select(\"url\",\"length\",\"nlines\",\"raw_content\",\"split_content\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd212cd6-25ca-434a-b633-01ca1e7b7ecd",
   "metadata": {},
   "source": [
    "### 4.3 将新字段展开获取paragraph级别row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70f305dc-6e04-40a9-8138-d3049dc4c070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "DataFrame[summary: string, url: string, date_download: string, digest: string, length: string, nlines: string, source_domain: string, title: string, raw_content: string, raw_line_id: string, raw_line: string]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----------+------------------------+\n",
      "|                 url|         raw_content|raw_line_id|                raw_line|\n",
      "+--------------------+--------------------+-----------+------------------------+\n",
      "|https://www.telel...|English\\tEnglish\\...|          0|    English\\tEnglish\\ten|\n",
      "|https://www.telel...|English\\tEnglish\\...|          1|繁體中文\\tChinese (Tr...|\n",
      "|https://www.telel...|English\\tEnglish\\...|          2|                    Home|\n",
      "|https://www.telel...|English\\tEnglish\\...|          3|                Products|\n",
      "|https://www.telel...|English\\tEnglish\\...|          4|              Digital TV|\n",
      "+--------------------+--------------------+-----------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:3.1675808429718018s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Explode the split_content column and select the desired columns\n",
    "exploded_df = split_result.select(\"url\",\"date_download\",\"digest\",\"length\",\"nlines\",\"source_domain\",\"title\",\"raw_content\", explode(split_result.split_content).alias(\"exploded_content\"))\n",
    "\n",
    "# Split the exploded_content struct into separate columns\n",
    "exploded_df = exploded_df.withColumn(\"raw_line_id\", exploded_df.exploded_content.raw_line_id)\n",
    "exploded_df = exploded_df.withColumn(\"raw_line\", exploded_df.exploded_content.raw_line)\n",
    "\n",
    "# Drop the exploded_content column if needed\n",
    "exploded_df = exploded_df.drop(\"exploded_content\")\n",
    "\n",
    "if(mode_para[\"isTest\"]):\n",
    "    exploded_df.cache()\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(exploded_df.summary())\n",
    "    exploded_df.select(\"url\",\"raw_content\",\"raw_line_id\",\"raw_line\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a82cec-45d4-4121-b570-c38091c425fd",
   "metadata": {},
   "source": [
    "### 4.4 添加hash 列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31d74e40-0f12-4556-bac2-0bd91c94cada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HASH_SIZE:8\n",
      "=== TestMode Log:\n",
      "DataFrame[summary: string, url: string, date_download: string, digest: string, length: string, nlines: string, source_domain: string, title: string, raw_content: string, raw_line_id: string, raw_line: string]\n",
      "+--------------------+--------------------+--------------------+------+------+----------------+--------------------+--------------------+-----------+------------------------+--------------------+\n",
      "|                 url|       date_download|              digest|length|nlines|   source_domain|               title|         raw_content|raw_line_id|                raw_line|          hash_value|\n",
      "+--------------------+--------------------+--------------------+------+------+----------------+--------------------+--------------------+-----------+------------------------+--------------------+\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4669|   111|www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|          0|    English\\tEnglish\\ten|[A7 E1 3C F2 70 F...|\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4669|   111|www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|          1|繁體中文\\tChinese (Tr...|[3E DB 9E EF B5 2...|\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4669|   111|www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|          2|                    Home|[E8 32 49 BD 3B A...|\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4669|   111|www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|          3|                Products|[FB DC 4F 23 F9 3...|\n",
      "|https://www.telel...|2019-02-15T19:35:48Z|sha1:VZYTYZZ7EH6E...|  4669|   111|www.telelynx.com|sean, Author at T...|English\\tEnglish\\...|          4|              Digital TV|[59 44 27 AB 00 F...|\n",
      "+--------------------+--------------------+--------------------+------+------+----------------+--------------------+--------------------+-----------+------------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:1.098217487335205s\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BinaryType\n",
    "from ccnet_spark import normalize_for_dedup\n",
    "from typing import Iterable, Iterator, Sequence, Sized, Tuple, Type\n",
    "HASH_TYPE: Type[np.uint64] = np.uint64\n",
    "HASH_SIZE = HASH_TYPE(0).nbytes \n",
    "print(f\"HASH_SIZE:{HASH_SIZE}\") # 8 Byte ==> 64bit\n",
    "@udf(returnType=BinaryType())\n",
    "def compute_hashes(line):\n",
    "    if not line:\n",
    "        return None\n",
    "    normalized_line = normalize_for_dedup(line)  # Assuming normalize_for_dedup is defined\n",
    "    line_hash = hashlib.sha1(bytes(normalized_line, encoding=\"utf-8\")).digest()[:HASH_SIZE]\n",
    "    return line_hash\n",
    "\n",
    "# Assuming you have a dataframe named 'df' with a 'raw_line' column\n",
    "hash_df = exploded_df.withColumn(\"hash_value\", compute_hashes(exploded_df.raw_line))\n",
    "\n",
    "# Show the resulting dataframe\n",
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    print(hash_df.summary())\n",
    "    hash_df.show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec0d5a4-2cd6-4925-8f53-690b0213bf9c",
   "metadata": {},
   "source": [
    "### 4.5根据 hash 去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78903f74-1113-483e-ba89-2987e1a26847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "+--------------------+------+------+--------------+\n",
      "|                 url|length|nlines|compute_length|\n",
      "+--------------------+------+------+--------------+\n",
      "|https://www.telel...|  4669|   111|          4669|\n",
      "|http://www.ma.hu/...|  3716|    70|          3716|\n",
      "|http://angagement...|  1231|    65|          1231|\n",
      "|http://resistther...|   868|    23|           868|\n",
      "|http://klimadiagr...|  1851|    51|          1851|\n",
      "+--------------------+------+------+--------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:0.602365255355835s\n"
     ]
    }
   ],
   "source": [
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s=time.time()\n",
    "    from pyspark.sql import functions as F\n",
    "    tmp_df = spark_df.withColumn(\"compute_length\", F.length(spark_df[\"raw_content\"]))\n",
    "    tmp_df.select(\"url\",\"length\",\"nlines\",\"compute_length\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "620aaf6b-9774-4a6f-98a2-962db1f9a9db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "DataFrame[summary: string, url: string, date_download: string, digest: string, length: string, nlines: string, source_domain: string, title: string, raw_content: string, raw_line_id: string, raw_line: string]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 10:46:23 WARN TaskSetManager: Stage 8 contains a task of very large size (1321 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+------------------------------+-----------+--------------------+\n",
      "|                 url|length|nlines|                   raw_content|raw_line_id|          hash_value|\n",
      "+--------------------+------+------+------------------------------+-----------+--------------------+\n",
      "|http://www.region...|  5765|   213|          Salta al contenut...|         22|[00 00 B7 6B E5 F...|\n",
      "|http://www.darulh...| 28049|   839|          Slå på/av meny\\nD...|        738|[00 02 2B BA 78 8...|\n",
      "|http://outdoormag...|  5695|   156|          O nas\\nRedakcja\\n...|        128|[00 02 FD 23 BA F...|\n",
      "|http://stk1031.bl...|  4265|   246|きまぐれあれやこれ\\n2018年5...|        167|[00 06 62 88 AC 5...|\n",
      "|https://jakandjil...|  3258|   305|          A moda está em tu...|         21|[00 08 83 89 3F 2...|\n",
      "+--------------------+------+------+------------------------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:57.8560049533844s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "deduplicated_df = hash_df.dropDuplicates(['hash_value'])\n",
    "# Show the resulting dataframe\n",
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    deduplicated_df.cache()\n",
    "    s=time.time()\n",
    "    print(deduplicated_df.summary())\n",
    "    deduplicated_df.select(\"url\",\"length\",\"nlines\",\"raw_content\",\"raw_line_id\",\"hash_value\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77ed5d3-f4b3-4d94-a663-6c9279071b5d",
   "metadata": {},
   "source": [
    "### 4.6 聚合\n",
    "将段落重新聚合为doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "48e9c8fc-fdd2-4d71-81ae-77b6ebf77805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 10:47:28 WARN MemoryStore: Not enough space to cache rdd_136_3 in memory! (computed 680.3 MiB so far)\n",
      "24/04/01 10:47:28 WARN MemoryStore: Not enough space to cache rdd_136_1 in memory! (computed 605.9 MiB so far)\n",
      "24/04/01 10:47:28 WARN BlockManager: Persisting block rdd_136_1 to disk instead.\n",
      "24/04/01 10:47:28 WARN BlockManager: Persisting block rdd_136_3 to disk instead.\n",
      "24/04/01 10:47:28 WARN MemoryStore: Not enough space to cache rdd_136_8 in memory! (computed 687.1 MiB so far)\n",
      "24/04/01 10:47:28 WARN BlockManager: Persisting block rdd_136_8 to disk instead.\n",
      "[Stage 11:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------+---------------+---------------------------+------+------+\n",
      "|                 url|original_length|original_nlines|                raw_content|length|nlines|\n",
      "+--------------------+---------------+---------------+---------------------------+------+------+\n",
      "|http://mylandia.r...|           1671|             88|       Отдушка Лунный цв...|  1143|    42|\n",
      "|http://www.recenz...|           4757|             46|       Tym, co niesamowi...|  4600|    35|\n",
      "|http://kuraruk.ha...|           1435|            100|9月の旅：東京編\\nKYOTO (...|  1062|    66|\n",
      "|https://www.govtj...|          15891|            386|       AP Fire Departmen...| 14328|   316|\n",
      "|http://www.millyb...|          23563|           1007|       View Shopping Car...| 15909|   250|\n",
      "+--------------------+---------------+---------------+---------------------------+------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "time consume:11.893906354904175s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "\"url\",\"date_download\",\"digest\",\"length\",\"nlines\",\"source_domain\",\"title\",\"raw_content\",\n",
    "group_df = deduplicated_df.groupBy(\"digest\").agg(\n",
    "    F.first(\"url\").alias(\"url\"),\n",
    "    F.first(\"date_download\").alias(\"date_download\"),\n",
    "    F.first(\"source_domain\").alias(\"source_domain\"),\n",
    "    F.first(\"length\").alias(\"original_length\"),\n",
    "    F.first(\"nlines\").alias(\"original_nlines\"),\n",
    "    F.first(\"title\").alias(\"title\"),\n",
    "    F.concat_ws(\"\\n\", F.collect_list(\"raw_line\").alias(\"raw_content\")).alias(\"raw_content\"),\n",
    "    F.count(\"raw_line_id\").alias(\"nlines\"),\n",
    "    F.collect_list(\"raw_line_id\").alias(\"line_ids\"),\n",
    ")\n",
    "group_df=group_df.withColumn(\"length\", F.length(group_df[\"raw_content\"]))\n",
    "if(mode_para[\"isTest\"]):\n",
    "    print(\"=== TestMode Log:\")\n",
    "    group_df.cache()\n",
    "    s=time.time()\n",
    "    group_df.select(\"url\",\"original_length\",\"original_nlines\",\"raw_content\",\"length\",\"nlines\").show(5)\n",
    "    e=time.time()\n",
    "    print(f\"time consume:{e-s}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59366d-6d0e-4ab8-937b-1dbcb39a1304",
   "metadata": {},
   "source": [
    "### 4.7 计算留存比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1295a70-8dce-40c1-aad2-103efb493626",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/01 10:47:32 WARN TaskSetManager: Stage 13 contains a task of very large size (1321 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TestMode Log:\n",
      "origin chars:30.588446M,remain_chars:24.302366999999997M \n",
      "             keep chars:79.449 %\n"
     ]
    }
   ],
   "source": [
    "if mode_para[\"isTest\"]:\n",
    "    print(\"=== TestMode Log:\")\n",
    "    s = time.time()\n",
    "    origin_chars = spark_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    remain_chars = group_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    e = time.time()\n",
    "    print(f\"origin chars:{origin_chars/1000/1000}M,remain_chars:{remain_chars/1000/1000}M \\n \\\n",
    "            keep chars:{round(remain_chars/origin_chars*100,3)} % time consume:{e-s}\")\n",
    "else:\n",
    "    print(\"=== DevMode Log:\")\n",
    "    s = time.time()\n",
    "    origin_chars = spark_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    remain_chars = group_df.agg(spark_sum(\"length\")).collect()[0][0]\n",
    "    e = time.time()\n",
    "    print(f\"origin chars:{origin_chars/1000/1000}M,remain_chars:{remain_chars/1000/1000}M \\n \\\n",
    "            keep chars:{round(remain_chars/origin_chars*100,3)} % time consume:{e-s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1063646-dc67-44d2-acfd-80be5ba32b88",
   "metadata": {},
   "source": [
    "## 5. 语言识别导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2886d4c-af9b-447b-a0a0-30eb23c04b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1fade9-7fa0-44b9-a922-98485fe229b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
